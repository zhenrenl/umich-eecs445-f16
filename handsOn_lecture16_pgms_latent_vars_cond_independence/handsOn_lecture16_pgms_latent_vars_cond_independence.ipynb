{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning \n",
    "\n",
    "Considering a model having\n",
    "- Observed variables $X$\n",
    "- Hidden variables $Z$\n",
    "- Parameters $\\theta$\n",
    "\n",
    "In lecture 16 we defined **Learning:**, estimate parameters $\\theta$ from observed data $X$, as follows:\n",
    "\n",
    "$$\n",
    "P(\\theta \\mid X) = \\sum_{z \\in Z} P(\\theta, z \\mid X) = \\sum_{z \\in Z} P(\\theta \\mid z, X) P(z \\mid X)\n",
    "$$\n",
    "\n",
    "Make sure you understand this expression: use repeated applications of bayes rule to show that:\n",
    "\n",
    "$ P(\\theta, z \\mid X) = P(\\theta \\mid z, X) P(z \\mid X) $\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional independence and Graphical Models\n",
    "\n",
    "In the following exercises, we ask whether variables in a graphical model are independent, perhaps conditioned on other variables. In each case, justify your answer by finding an \"active path\" or showing no such active path exists (e.g show D-separation) as appropriate.\n",
    "\n",
    "For example, if we as whether $A \\perp B \\mid C$ you could answer no by finding an active path from $A$ to $B$, or say that yes, they are indpendent by showing that $A$ and $B$ have D-separation (there are no active paths).\n",
    "\n",
    "**Q1**: is $A \\perp B$?\n",
    "\n",
    "<img src=\"ind1.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q2**: is $A \\perp B \\mid E$?\n",
    "\n",
    "<img src=\"ind1-observed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q3**: is $A \\perp B \\mid C$?\n",
    "\n",
    "<img src=\"ind2-observed-c.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q4**: is $A \\perp B \\mid F$?\n",
    "\n",
    "<img src=\"ind2-observed-f.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q5**: is $X_4 \\perp \\left\\{ {X_1, X_3} \\right\\} \\mid X_2$?\n",
    "\n",
    "<img src=\"ind3-observed-x2.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q6**: is $X_1 \\perp X_6 \\mid \\left\\{ {X_2, X_3} \\right\\} $?\n",
    "\n",
    "<img src=\"ind3-observed-x2x3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Q7**: is $X_2 \\perp X_3 \\mid \\left\\{ {X_1, X_6} \\right\\} $?\n",
    "\n",
    "<img src=\"ind3-observed-x1x6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "<img src=\"../lecture15_exp_families_bayesian_networks/images/hmm.png\">\n",
    "\n",
    "Noisy observations $X_k$ generated from hidden Markov chain $Y_k$.\n",
    "$$\n",
    "P(\\vec{X}, \\vec{Y}) = P(Y_1) P(X_1 \\mid Y_1) \\prod_{k=2}^N \\left(P(Y_k \\mid Y_{k-1}) P(X_k \\mid Y_k)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Find the most likely sequence\n",
    "\n",
    "Assume you are given the parameters of the HMM model, i.e. $\\pi_y = P(Y_1 = y)$, $B_{i,j} = P(X_k = j \\mid Y_k = i)$, and $A_{i,j} = P(Y_k = j \\mid Y_{k-1} = i)$.\n",
    "\n",
    "Assume you are given a sequence of observed variables $X_1, \\ldots, X_T$. Can you figure out how to compute the *most likely sequence* of unobserved variables? That is, how can we compute:\n",
    "$$ \\arg\\max_{Y_1, \\ldots, Y_T} P(Y_1, \\ldots, Y_T \\mid X_1, \\ldots, X_T) $$\n",
    "\n",
    "*Hint*: Dynamic programming. Try to recursively compute $V_{t}(i)$, which is the probability (given all of the observed data) of the most likely (sub)sequence $Y_1, \\ldots, Y_t$ *that ends in* $Y_t = i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
