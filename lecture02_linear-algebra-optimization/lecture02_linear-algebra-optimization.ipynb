{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EECS 445 - Introduction to Machine Learning\n",
    "## Lecture 2: Linear Algebra and Optimization\n",
    "### Date: September 12, 2016\n",
    "### Instructor: Jacob Abernethy and Jia Deng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML, Image\n",
    "from IPython.display import YouTubeVideo\n",
    "from sympy import init_printing, Matrix, symbols, Rational\n",
    "import sympy as sym\n",
    "from warnings import filterwarnings\n",
    "init_printing(use_latex = 'mathjax')\n",
    "filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TODAY: Fast overview of linear algebra + convexity\n",
    "\n",
    "we're going to do:\n",
    "* Vectors and norms\n",
    "* Matrices\n",
    "* Positive definite matrices\n",
    "* Eigendecomposition\n",
    "* Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a11, a12, a13, a21, a22, a23, a31, a32, a33, b11, b12, b13, b21, b22, b23, b31, b32, b33 = symbols('a11 a12 a13 a21 a22 a23 a31 a32 a33 b11 b12 b13 b21 b22 b23 b31 b32 b33')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}a_{11} & a_{12} & a_{13}\\\\a_{21} & a_{22} & a_{23}\\end{matrix}\\right], \\quad \\left[\\begin{matrix}b_{11} & b_{12}\\\\b_{21} & b_{22}\\\\b_{31} & b_{32}\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡a₁₁  a₁₂  a₁₃⎤, ⎡b₁₁  b₁₂⎤⎞\n",
       "⎜⎢             ⎥  ⎢        ⎥⎟\n",
       "⎜⎣a₂₁  a₂₂  a₂₃⎦  ⎢b₂₁  b₂₂⎥⎟\n",
       "⎜                 ⎢        ⎥⎟\n",
       "⎝                 ⎣b₃₁  b₃₂⎦⎠"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Matrix([[a11, a12, a13], [a21, a22, a23]])\n",
    "B = Matrix([[b11, b12], [b21, b22], [b31, b32]])\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}a_{11} b_{11} + a_{12} b_{21} + a_{13} b_{31} & a_{11} b_{12} + a_{12} b_{22} + a_{13} b_{32}\\\\a_{21} b_{11} + a_{22} b_{21} + a_{23} b_{31} & a_{21} b_{12} + a_{22} b_{22} + a_{23} b_{32}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡a₁₁⋅b₁₁ + a₁₂⋅b₂₁ + a₁₃⋅b₃₁  a₁₁⋅b₁₂ + a₁₂⋅b₂₂ + a₁₃⋅b₃₂⎤\n",
       "⎢                                                        ⎥\n",
       "⎣a₂₁⋅b₁₁ + a₂₂⋅b₂₁ + a₂₃⋅b₃₁  a₂₁⋅b₁₂ + a₂₂⋅b₂₂ + a₂₃⋅b₃₂⎦"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = A * B\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}4 & 10\\\\4 & 14\\end{matrix}\\right], \\quad \\left[\\begin{matrix}5 & 7\\\\7 & 13\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡4  10⎤, ⎡5  7 ⎤⎞\n",
       "⎜⎢     ⎥  ⎢     ⎥⎟\n",
       "⎝⎣4  14⎦  ⎣7  13⎦⎠"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Matrix([[3, 1], [1, 3]])\n",
    "B = Matrix ([[1, 2], [1,4]])\n",
    "C1 = A * B\n",
    "C2 = B * A\n",
    "C1, C2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Transpose\n",
    "* The transpose $A^T$ of a matrix $A$ is what you get from \"swapping\" rows and columns\n",
    "$$A \\in \\mathbb{R}^{n \\times m} \\implies A^T \\in  \\mathbb{R}^{m \\times n}$$\n",
    "$$(A^T)_{i,j} := A_{j,i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}1 & 2\\\\3 & 4\\\\5 & 6\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1 & 3 & 5\\\\2 & 4 & 6\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1 & 3\\\\2 & 4\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡1  2⎤, ⎡1  3  5⎤, ⎡1  2⎤, ⎡1  3⎤⎞\n",
       "⎜⎢    ⎥  ⎢       ⎥  ⎢    ⎥  ⎢    ⎥⎟\n",
       "⎜⎢3  4⎥  ⎣2  4  6⎦  ⎣3  4⎦  ⎣2  4⎦⎟\n",
       "⎜⎢    ⎥                           ⎟\n",
       "⎝⎣5  6⎦                           ⎠"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Matrix ([[1, 2], [3,4], [5,6]])\n",
    "B = Matrix ([[1, 2], [3,4]])\n",
    "A, A.transpose(), B, B.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A matrix $A$ is *symmetric* if we have $A^\\top = A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}1 & 2\\\\2 & 1\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1 & 2\\\\2 & 1\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡1  2⎤, ⎡1  2⎤⎞\n",
       "⎜⎢    ⎥  ⎢    ⎥⎟\n",
       "⎝⎣2  1⎦  ⎣2  1⎦⎠"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Matrix ([[1, 2], [2,1]])\n",
    "A, A.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Some easy ways to get a symmetric matrix:\n",
    "$$A + A^\\top, \\quad A A^\\top, \\quad A^\\top A $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transpose properties\n",
    "* Obvious properties of the transpose:\n",
    "    * $(A + B)^T = A^T + B^T$\n",
    "    * $(AB)^T = A^T B^T$ (......right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* No! Careful!\n",
    "    * $(AB)^T = B^T A^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rank of a matrix\n",
    "\n",
    "* Linear independent vectors: no vector can be represented as a linear combination of other vectors. \n",
    "* rank($A$) (the rank of a m-by-n matrix $A$) is\n",
    "    * The maximal number of linearly independent columns = The maximal number of linearly independent rows\n",
    "    \n",
    "* col($A$), the column space of a m-by-n matrix $A$, is the set of all possible linear combinations of its column vectors.\n",
    "* row($A$), the row space of a m-by-n matrix $A$, is the set of all possible linear combinations of its row vectors.\n",
    "\n",
    "* rank($A$) = dimension of col($A$) = dimension of row($A$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can still talk about Rank for non-square matrices\n",
    "* If $A$ is n by m, then\n",
    "    * rank($A$) ≤ min(m,n)\n",
    "    * If rank($A$) = n, then $A$ has full row rank\n",
    "    * If rank($A$) = m, then $A$ has full column rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Norms\n",
    "* A norm measures the \"length\" of a vector\n",
    "* We usually use notation $\\|x\\|$ to denote the norm of $x$\n",
    "* A norm is a function $f : \\mathbb{R}^n \\to \\mathbb{R}$ such that:\n",
    "    * $f(x) \\geq 0$ for all $x$\n",
    "    * $f(x) = 0 \\Longleftrightarrow x = 0$\n",
    "    * $f(tx) = |t| f(x)$ for all $x$\n",
    "    * $f(x + y) \\leq f(x) + f(y)$ for all $x$ and $y$ (Triangle Inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of norms\n",
    "* Perhaps the most common norm is the Euclidean norm\n",
    "$$ \\|x\\|_2 := \\sqrt{x_1^2 + x_2^2 + \\ldots x_n^2}$$\n",
    "\n",
    "* This is a special case of the $p$-norm:\n",
    "$$ \\|x\\|_p := \\left(|x_1|^p + \\ldots + |x_n|^p\\right)^{1/p}$$\n",
    "\n",
    "* There's also the so-called **infinity norm**\n",
    "$$ \\|x\\|_\\infty := \\max_{i=1,\\ldots, n} |x_i|$$\n",
    "\n",
    "* A vector $x$ is said to be *normalized* if $\\|x\\| = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix inversion\n",
    "* The inverse $A^{-1}$ of a *square matrix* $A$ is the unique matrix such that $A A^{-1} = A^{-1}A = I$\n",
    "* The inverse doesn't always exist! (For example, when $A$ not full-rank)\n",
    "* If $A$ and $B$ are invertible, then $AB$ is invertible and $(AB)^{-1} = B^{-1} A^{-1}$\n",
    "* If $A$ is invertible, then $A^T$ is invertible and $(A^T)^{-1} = (A^{-1})^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}3.0 & 1.0\\\\1.0 & 3.0\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡3.0  1.0⎤\n",
       "⎢        ⎥\n",
       "⎣1.0  3.0⎦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[3, 1], [1, 3]])\n",
    "Matrix(X) # putting Matrix() around X is just for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}0.375 & -0.125\\\\-0.125 & 0.375\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1.0 & 0.0\\\\0.0 & 1.0\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1.0 & 0.0\\\\0.0 & 1.0\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡0.375   -0.125⎤, ⎡1.0  0.0⎤, ⎡1.0  0.0⎤⎞\n",
       "⎜⎢              ⎥  ⎢        ⎥  ⎢        ⎥⎟\n",
       "⎝⎣-0.125  0.375 ⎦  ⎣0.0  1.0⎦  ⎣0.0  1.0⎦⎠"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xinv = np.linalg.inv(X)\n",
    "Matrix(Xinv), Matrix(Xinv.dot(X)), Matrix(X.dot(Xinv)) # Should give the identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Determinant + Trace\n",
    "\n",
    "* The determinant of a **square** matrix $A$, denoted $|A|$, has the following recursive structure:\n",
    "$$\\begin{vmatrix} a & b & c & d\\\\e & f & g & h\\\\i & j & k & l\\\\m & n & o & p \\end{vmatrix}=a\\begin{vmatrix} f & g & h\\\\j & k & l\\\\n & o & p \\end{vmatrix} \n",
    "- b\\begin{vmatrix} e & g & h\\\\i & k & l\\\\m & o & p \\end{vmatrix}\\\\\n",
    "+c\\begin{vmatrix} e & f & h\\\\i & j & l\\\\m & n & p \\end{vmatrix}\n",
    "-d\\begin{vmatrix} e & f & g\\\\i & j & k\\\\m & n & o \\end{vmatrix}.\n",
    "$$\n",
    "* $|A| \\neq 0$ if and only if $A$ is invertible (non-singular). \n",
    "\n",
    "* The trace of a matrix, denoted tr$(A)$, is defined as the sum of the diagonal elements of $A$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal + Normalized = Orthonormal\n",
    "* Two vectors $x,y$ are *orthogonal* if $x^T y = 0$\n",
    "* A square matrix $U \\in \\mathbb{R}^{n \\times n}$ is *orthogonal* if all columns $U_1, \\ldots, U_n$ are orthogonal to each other (i.e. $U_i^\\top U_j = 0$ for $i \\ne j$)\n",
    "* $U$ is *orthonormal* if it is orthogonal **and** the columns are normalized, i.e. $\\|U_i\\|_2 = 1$ for every $i$.\n",
    "* If $U$ is orthonormal, then $U^T U = I$, that is, $U^{-1} = U^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positive Definiteness\n",
    "* We say a symmetric matrix $A$ is positive definite if\n",
    "$$ x^\\top A x > 0 \\text{ for all } x \\ne 0$$\n",
    "* We say a matrix is positive semi-definite (PSD) if \n",
    "$$ x^\\top A x \\geq 0 \\text{ for all } x$$\n",
    "* A matrix that is positive definite gives us a *norm*. Let\n",
    "$$ \\| x \\|_A := \\sqrt{x ^\\top A x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "### What are eigenvectors?\n",
    "* A Matrix is a mathematical object that acts on a (column) vector, resulting in a new vector, i.e. A**x**=**b**\n",
    "* An eigenvector is *non-zero* vector such that the resulting vector is parallel to **x** (some multiple of **x**)\n",
    "$$ {A}\\underline{x}=\\lambda \\underline{x} $$\n",
    "* $\\lambda$ is called an eigenvalue. \n",
    "* The eigenvectors with an eigenvalue of zero are the vectors in the nullspace of $A$. \n",
    "* If A is singular (takes some non-zero vector into 0) then zero is an eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lamda = symbols('lamda') # Note that lambda is a reserved word in python, so we use lamda (without the b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to solve A**x**=&#955;**x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ A\\underline { x } =\\lambda \\underline { x } \\\\ \\left( A-\\lambda I \\right) \\underline { x } =\\underline { 0 }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The only solution to this equation is for A-&#955;I to be singular and therefor have a determinant of zero\n",
    "$$ \\left|{A}-\\lambda{I}\\right|=0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $|A-\\lambda I|$ is a polynomial of the variable $\\lambda$ and is called the characteristic polynomial of $A$. \n",
    "* The eigenvalues are the roots of the equation of $ \\left|{A}-\\lambda{I}\\right|=0 $. They may be complex numbers. \n",
    "* There will be *n* $\\lambda$'s for an $n\\times n$ matrix (some of which may be of equal value) \n",
    "* Given an eigenvalue $\\lambda$, its eigenvectors are the null space of $A-\\lambda I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example eigenvalue problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}3 & 1\\\\1 & 3\\end{matrix}\\right], \\quad \\left[\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡3  1⎤, ⎡1  0⎤⎞\n",
       "⎜⎢    ⎥  ⎢    ⎥⎟\n",
       "⎝⎣1  3⎦  ⎣0  1⎦⎠"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Matrix([[3, 1], [1, 3]])\n",
    "I = sym.eye(2)\n",
    "A, I # Printing A and the 2-by-2 identity matrix to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}- \\lambda + 3 & 1\\\\1 & - \\lambda + 3\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡-λ + 3    1   ⎤\n",
       "⎢              ⎥\n",
       "⎣  1     -λ + 3⎦"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A - lamda * I) # Printing A minus lambda times the identity matrix to the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This will have the following (symbolic) determinant polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\lambda^{2} - 6 \\lambda + 8$$"
      ],
      "text/plain": [
       " 2          \n",
       "λ  - 6⋅λ + 8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A - lamda * I).det()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalue example\n",
    "* We can solve the polynomial $\\lambda^2 - 6\\lambda + 8$ with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left(\\lambda - 4\\right) \\left(\\lambda - 2\\right)$$"
      ],
      "text/plain": [
       "(λ - 4)⋅(λ - 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((A - lamda * I).det()).factor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I now have two eigenvalues of 2 and 4\n",
    "* I can get the two eigenvectors, $x_1$ and $x_2$, by solving\n",
    "$$ (A - 2I)x_1 = 0 \\text{ and } (A - 4I)x_2 = 0 $$\n",
    "* I need to find a vector in the *null space* of $A - 2I$ and $A - 4I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting eigenvalues/vectors using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}3.0 & 1.0\\\\1.0 & 3.0\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡3.0  1.0⎤\n",
       "⎢        ⎥\n",
       "⎣1.0  3.0⎦"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[3, 1], [1, 3]])\n",
    "Matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}0.707106781186547 & -0.707106781186547\\\\0.707106781186547 & 0.707106781186547\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡0.707106781186547  -0.707106781186547⎤\n",
       "⎢                                     ⎥\n",
       "⎣0.707106781186547  0.707106781186547 ⎦"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvals, eigenvecs = np.linalg.eig(X)\n",
    "Matrix(eigenvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}4.0\\\\2.0\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡4.0⎤\n",
       "⎢   ⎥\n",
       "⎣2.0⎦"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(eigenvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trace related to eigenvals\n",
    "* Let $A$ be a matrix whose eigenvalues are $\\lambda_1, \\ldots, \\lambda_n$\n",
    "* Then we have the trace of satisfying\n",
    "$$\\text{tr}(A) = \\sum_{i=1}^n \\lambda_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( 56.1848049439, \\quad 56.1848049439\\right )$$"
      ],
      "text/plain": [
       "(56.1848049439, 56.1848049439)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(5,10)\n",
    "A = X.dot(X.T) # For fun, let's look at A = X * X^T\n",
    "eigenvals, eigvecs = np.linalg.eig(A) # Compute eigenvalues of A\n",
    "sum_of_eigs = sum(eigenvals) # Sum the eigenvalues\n",
    "trace_of_A = A.trace() # Look at the trace\n",
    "(sum_of_eigs, trace_of_A) # Are they the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Determinant related to eigenvals\n",
    "* Let $A$ be a matrix whose eigenvalues are $\\lambda_1, \\ldots, \\lambda_n$\n",
    "* Then we have the trace of satisfying\n",
    "$$|A| = \\prod_{i=1}^n \\lambda_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( 21358.046809, \\quad 21358.046809\\right )$$"
      ],
      "text/plain": [
       "(21358.046809, 21358.046809)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll use the same matrix A as before\n",
    "prod_of_eigs = np.prod(eigenvals) # Sum the eigenvalues\n",
    "determinant = np.linalg.det(A) # Look at the trace\n",
    "(prod_of_eigs, determinant) # Are they the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Singular Value Decomposition\n",
    "* Any matrix (symmetric, non-symmetric, etc.) $A \\in \\mathbb{R}^{n\\times m}$ admits a *singular value decomposition* (SVD)\n",
    "* The decomposition has three factors, $U \\in \\mathbb{R}^{n \\times n}$, $\\Sigma \\in \\mathbb{R}^{n \\times m}$, and $V \\in \\mathbb{R}^{m \\times m}$\n",
    "$$A = U \\Sigma V^\\top$$\n",
    "* $U$ and $V$ are both orthonormal matrices, and $\\Sigma$ is diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}4.0 & 4.0\\\\-3.0 & 3.0\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡4.0   4.0⎤\n",
       "⎢         ⎥\n",
       "⎣-3.0  3.0⎦"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[4, 4], [-3, 3]])\n",
    "Matrix(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let's show Sigma from the SVD output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}5.65685424949238 & 0.0\\\\0.0 & 4.24264068711928\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡5.65685424949238        0.0       ⎤\n",
       "⎢                                  ⎥\n",
       "⎣      0.0         4.24264068711928⎦"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, Sigma_diags, V = np.linalg.svd(A)\n",
    "Matrix(np.diag(Sigma_diags)) # Numpy's SVD only returns diagonals, here I'm showing full Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* And we can show the orthonormal bases $U$ and $V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "U,V = np.round(U,decimals=5), np.round(V,decimals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}-1.0 & 0.0\\\\0.0 & 1.0\\end{matrix}\\right], \\quad \\left[\\begin{matrix}-0.70711 & -0.70711\\\\-0.70711 & 0.70711\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡-1.0  0.0⎤, ⎡-0.70711  -0.70711⎤⎞\n",
       "⎜⎢         ⎥  ⎢                  ⎥⎟\n",
       "⎝⎣0.0   1.0⎦  ⎣-0.70711  0.70711 ⎦⎠"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(U), Matrix(V) # I rounded the values for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of the SVD\n",
    " $$\\text{SVD:} \\quad \\quad A = U \\Sigma V^\\top$$\n",
    "* The *singular values* of $A$ are the diagonal elements of $\\Sigma$\n",
    "* The singular values of $A$ are the *square roots of the eigenvalues* of both $A^\\top A$ and $A A^\\top$\n",
    "* The *left-singular vectors* of $A$, i.e. the columns of $U$, are the *eigenvectors* of $A A^\\top$\n",
    "* The *right-singular vectors* of $A$, i.e. the columns of $V$, are the *eigenvectors* of $A^\\top A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $$M = U\\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Singular_value_decomposition.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/e/e9/Singular_value_decomposition.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wikipedia: Visualization of the SVD of a 2d matrix M. First, we see the unit disc in blue together with the two canonical unit vectors. We then see the action of M, which distorts the disk to an ellipse. The SVD decomposes M into three simple transformations: an initial rotation V∗, a scaling Σ along the coordinate axes, and a final rotation U. The lengths σ1 and σ2 are singular values of M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functions and Convexity\n",
    "* Let $f$ be a function mapping $\\mathbb{R}^{n} \\to \\mathbb{R}$, and assume $f$ is twice differentiable.\n",
    "* The *gradient* and *hessian* of $f$, denoted $\\nabla f(x)$ and $\\nabla^2 f(x)$, are the vector an matrix functions:\n",
    "$$\\nabla f(x) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}\\end{bmatrix} \\quad \\quad \\quad \\nabla^2 f(x) = \\begin{bmatrix}\\frac{\\partial^2 f}{\\partial x_1^2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\  \\vdots & & \\vdots \\\\\\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2}\\end{bmatrix}$$\n",
    "* Note: the hessian is always symmetric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradients of three simple functions\n",
    "* Let $b$ be some vector, and $A$ be some matrix\n",
    "* $f(x) = b^\\top x \\implies \\nabla_x f(x) = b$\n",
    "* $f(x) = x^\\top A x \\implies \\nabla_x f(x) = 2 A x$\n",
    "* $f(x) = x^\\top A x \\implies \\nabla^2_x f(x) = 2 A $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex functions\n",
    "* We say that a function $f$ is *convex* if, for any distinct pair of points $x,y$ we have\n",
    "$$f\\left(\\frac{x + y}{2}\\right) \\leq \\frac{f(x)}{2} + \\frac{f(y)}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.probabilitycourse.com/images/chapter6/Convex_b.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://www.probabilitycourse.com/images/chapter6/Convex_b.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fun facts about convex functions\n",
    "* If $f$ is differentiable, then $f$ is convex iff $f$ \"lies above its linear approximation\", i.e.:\n",
    "$$ f(x + y) \\geq f(x) + \\nabla_x f(x) \\cdot y \\quad \\text{ for every } x,y$$\n",
    "* If $f$ is twice-differentiable, then the hessian is always positive semi-definite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## See you all on Wednesday!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
