{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\text{LaTeX command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 445:  Machine Learning\n",
    "## Hands On 05:  Linear Regression II\n",
    "* Instructor:  **Zhao Fu, Valli, Jacob Abernethy and Jia Deng**\n",
    "* Date:  September 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Maximum Likelihood\n",
    "Suppose we have a set of observed data $D$ and we want to evaluate a parameter setting $w$:\n",
    "$$p(w|D) = \\frac{p(D|w)p(w)}{p(D)}$$\n",
    "$$p(D) = \\sum_{w}{p(D|w)p(w)}$$\n",
    "\n",
    "We call $p(D|w)$ as the likelihood function. Then we have $p(w|D) \\propto p(D|w)p(w)$. Suppose $p(w)$ is the same for all $w$, we can only choose $w$ to maximize likelihood $p(D|w)$, which is to maximize the log-likelihood $\\log{p(D|w)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review Problem: Maximum Likelihood Estimation\n",
    "We have observed data $x_1, \\cdots, x_n$ drawn from Bernoulli distribution:\n",
    "$$p(x) = \\begin{cases}\n",
    "    \\theta     & \\quad \\text{if } x = 1\\\\\n",
    "    1 - \\theta  & \\quad \\text{if } x = 0\\\\\n",
    "  \\end{cases}$$\n",
    "\n",
    "(a) What is the likelihood function based on $\\theta$?\n",
    "\n",
    "(b) What is the log-likelihood function?\n",
    "\n",
    "(c) Compute estimated $\\theta$ to maximize the log-likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Maximum Likelihood\n",
    "Suppose we have a set of observed data $D$ and we want to evaluate a parameter setting $w$:\n",
    "$$p(w|D) = \\frac{p(D|w)p(w)}{p(D)}$$\n",
    "$$p(D) = \\sum_{w}{p(D|w)p(w)}$$\n",
    "\n",
    "We call $p(D|w)$ as the likelihood function. Then we have $p(w|D) \\propto p(D|w)p(w)$. Suppose $p(w)$ is the same for all $w$, we can only choose $w$ to maximize likelihood $p(D|w)$, which is to maximize the log-likelihood $\\log{p(D|w)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 1: Maximum Likelihood Estimation\n",
    "(a) $$\n",
    "\\begin{array}{ll}\n",
    "L(\\theta) &= p(D|\\theta) = p(x_1, \\dots, x_n|\\theta) \\\\\n",
    "&= \\prod_{i}{p(x_i)} = \\theta^{\\sum{\\mathbb{1}(x_i = 1)}}(1 - \\theta)^{\\sum{\\mathbb{1}(x_i = 0)}} \\\\\n",
    "&= \\theta^{k}(1 - \\theta)^{n - k}\n",
    "\\end{array}\n",
    "$$\n",
    "where $k$ is the number of $1$s from the observed data.\n",
    "\n",
    "(b) $$\\log{L(\\theta)} = k\\log(\\theta) + (n - k)\\log(1 - \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 1: Maximum Likelihood Estimation\n",
    "(c) Set the derivative of $log(L(\\theta))$ to zero we have\n",
    "$$\n",
    "\\frac{\\mathrm{d}log(L(\\theta))}{\\mathrm{d}\\theta} = \\frac{k}{\\theta} - \\frac{n - k}{1 - \\theta} = 0 \\\\\n",
    "\\frac{k}{\\theta} = \\frac{n - k}{1 - \\theta} \\\\\n",
    "\\theta = \\frac{k}{n}\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 2: Maximum Likelihood Estimation for Gaussian Distribution\n",
    "We have observed data $x_1, \\cdots, x_n$ drawn from Normal distribution:\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^\\frac{1}{2}} \\exp{(-\\frac{1}{2\\sigma^2}(x - \\mu)^2)}$$\n",
    "\n",
    "(a) What is the likelihood function based on $\\mu$ and $\\sigma^2$?\n",
    "\n",
    "(b) What is the log-likelihood function?\n",
    "\n",
    "(c) Compute estimated parameters $\\mu$ and $\\sigma^2$ to maximize the log-likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 2\n",
    "We have observed data $x_1, \\cdots, x_n$ drawn from Normal distribution:\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^\\frac{1}{2}} \\exp{(-\\frac{1}{2\\sigma^2}(x - \\mu)^2)}$$\n",
    "\n",
    "(a) What is the log-likelihood function?\n",
    "\n",
    "**Answer**: $-(n/2)\\log \\sigma - \\sum_{i=1}^n\\frac{1}{2\\sigma^2}(x_i - \\mu)^2$\n",
    "\n",
    "(b) Compute estimated parameters $\\mu$ and $\\sigma^2$ to maximize the log-likelihood function.\n",
    "\n",
    "**Answer**: \n",
    "* $\\mu_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i$\n",
    "* $\\sigma^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu_{\\text{MLE}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Least Squares: Objective Function\n",
    "\n",
    "- Recall the objective function we minimizes in last lecture is \n",
    "$$\n",
    "E(\\vec{w}) = \\frac12 \\sum_{n=1}^N \\left( \\vec{w}^T \\phi(\\vec{x}_n) - t_n \\right)^2\n",
    "$$\n",
    "\n",
    "- To penalize the large coefficients, we will add one penalization/regularization term to it and minimize them altogether.\n",
    "$$\n",
    "E(\\vec{w}) = \\underbrace{ \\frac12 \\sum_{n=1}^N \\left( \\vec{w}^T \\phi(\\vec{x}_n) - t_n \\right)^2 }_{E_D(\\vec{w})}+ \\underbrace{\\boxed{\\frac{\\lambda}{2} \\left \\| \\vec{w} \\right \\|^2}}_{E_W(\\vec{w})}\n",
    "$$\n",
    "of which $E_D(\\vec{w})$ represents the term of sum of squared errors and $E_W(\\vec{w})$ is the regularization term.\n",
    "\n",
    "- $\\lambda$ is the regularization coefficient. \n",
    "- If $\\lambda$ is large, $E_{\\vec{W}}(\\vec{w})$ will dominate the objective function. As a result we will focus more on minimizing $E_W(\\vec{w})$ and the resulting solution $\\vec{w}$ tends to have smaller norm and the $E_D(\\vec{w})$ term will be larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Least Squares: Derivation\n",
    "- Based on what we have derived in last lecture, we could write the objective function as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(\\vec{w}) \n",
    "&= \\frac12 \\sum_{n=1}^N \\left( \\vec{w}^T \\phi(\\vec{x}_n) - t_n \\right)^2 + \\frac{\\lambda}{2} \\left \\| \\vec{w} \\right \\|^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Exercise**: Derive the gradient in element-wise to verify the above result, i.e. using $\\phi(\\vec{x}_n)_d$ and $w_d$ to represent $E(w_1, w_2, \\dots, w_D)$ and derive $\\frac{\\partial E}{\\partial w_d}$. Suppose $\\phi(\\vec{x_n}) \\in \\mathbb{R}^D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Regularized Least Squares: Solution\n",
    "- Based on what we have derived in last lecture, we could write the objective function as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(\\vec{w}) = \\frac{1}{2}\\sum_{n = 1}^{N}{(\\sum_{d=1}^{D}{w_d\\phi_d(\\vec{x}_n)} - t_n)^2} + \\frac{\\lambda}{2}\\sum_{d=1}^{D}{w_d^2} \\\\\n",
    "\\frac{\\partial E}{\\partial w_d} = \\sum_{n = 1}^{N}{\\phi_d(\\vec{x}_n)(\\sum_{d=1}^{D}{w_d\\phi_d(\\vec{x}_n)} - t_n)} + \\lambda w_d \\\\\n",
    "\\frac{\\partial E}{\\partial w_d} = \\sum_{n = 1}^{N}{\\phi_d(\\vec{x}_n)(\\vec{w}^T\\phi(\\vec{x}_n) - t_n)} + \\lambda w_d\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The gradient is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\vec{w}} E(\\vec{w}) \n",
    "&= \\Phi^T \\Phi \\vec{w} - \\Phi^T \\vec{t} + \\lambda \\vec{w}\\\\\n",
    "&= (\\Phi^T \\Phi + \\lambda I)\\vec{w} - \\Phi^T \\vec{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Setting the gradient to 0, we will get the solution\n",
    "$$\n",
    "\\boxed{ \\hat{\\vec{w}}=(\\Phi^T \\Phi + \\lambda I)^{-1} \\Phi^T \\vec{t} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Least Squares: Closed Form\n",
    "\n",
    "In the solution to ordinary least squares which is $\\hat{\\vec{w} }=(\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{t}$, we cannot guarantee $\\Phi^T \\Phi$ is invertible. But in regularized least squares, if $\\lambda > 0$, $\\Phi^T \\Phi + \\lambda I$ is always invertible.\n",
    "\n",
    "**Exercise**: To be invertible, a matrix needs to be full rank. Argue that $\\Phi^T \\Phi + \\lambda I$ is full rank by characterizing its $p$ eigenvalues in terms of the singular values of $\\Phi$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution:\n",
    "Suppose $\\Phi = U^T\\Lambda V$ which is SVD of $\\Phi$, we have $\\Phi^T\\Phi = V^T\\Lambda^2V$. \n",
    "\n",
    "Then we have $(\\Phi^T\\Phi + \\lambda I)V^T = V^T(\\Lambda^2 + \\lambda I)$. \n",
    "\n",
    "The $i^{th}$ eigenvalue of $(\\Phi^T\\Phi + \\lambda I)$ is $\\lambda_i^2 + \\lambda > 0$ where $\\lambda_i$ is the singular value for $\\Phi$. \n",
    "\n",
    "Then $\\det{(\\Phi^T\\Phi + \\lambda I)} = \\prod{(\\lambda_i^2 + \\lambda)} > 0$, which means $\\Phi^T\\Phi + \\lambda I$ is invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Least Squares:  Different Norms\n",
    "- The $\\ell^p$ norm of a vector $\\vec{x}$ is defined as\n",
    "$$\n",
    "\\left \\| \\vec{x} \\right \\|_p = (\\sum_{j=1}^{M} |x_j|^p)^\\frac{1}{p}\n",
    "$$\n",
    "\n",
    "- For the regularized least squares above, we used $\\ell^2$ norm. We could also use other $\\ell^p$ norms for different regularizers and the objective function becomes\n",
    "$$\n",
    "E(\\vec{w}) = \\frac12 \\sum_{n=1}^N \\left( \\vec{w}^T \\phi(\\vec{x}_n) - t_n \\right)^2 + \\frac{\\lambda}{2} \\left \\| \\vec{w} \\right \\|_p^p\n",
    "$$\n",
    "\n",
    "**Exercise**: Derive the element-wise gradient for the above $\\ell^p$ norm regularized energy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Least Squares:  Summary\n",
    "\n",
    "- Simple modification of linear regression\n",
    "- $\\ell^2$ Regularization controls the tradeoff between *fitting error* and *complexity*.\n",
    "    - Small $\\ell^2$ regularization results in complex models, but with risk of overfitting\n",
    "    - Large $\\ell^2$ regularization results in simple models, but with risk of underfitting\n",
    "- It is important to find an optimal regularization that *balances* between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probablistic Interpretation of Least Squares Regression\n",
    "- We have showed derived the solution to least squares regression by minimizing objective function. Now we will provide a probablistic perspective. Specifically, we will show the solution to **regular least squares** is just the **maximum likelihood** estimate of $\\vec{w}$ and the solution to **regularized least squares** is the **Maximum a Posteriori** estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Background\n",
    "- Gaussian Distribution\n",
    "$$ \n",
    "\\mathcal{N}(x, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ \\frac{(x-\\mu)^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "- **Maximum Likelihood Estimation** and **Maximum a Posteriori Estimation (MAP)**\n",
    "    - For distribution $t \\sim p(t|\\theta)$. $\\theta$ is some unknown parameter (like mean or variance) to be estimated.\n",
    "    - Given observation $\\vec{t} = (t_1, t_2, \\dots, t_N)$, \n",
    "        - The Maximum Likelihood Estimator is\n",
    "        $$\n",
    "        \\theta_{ML} = \\arg \\max \\prod_{n=1}^N p(t_n | \\theta)\n",
    "        $$\n",
    "        - If we have some prior knowledge about $\\theta$, the MAP estimator is \n",
    "        $$\n",
    "        \\theta_{MAP} = \\arg \\max \\prod_{n=1}^N p(\\theta | t_n) \\quad (\\text{Posteriori Probability of } \\theta)\n",
    "        $$        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimator $\\vec{w}_{ML}$\n",
    "\n",
    "- We assume the **signal+noise** model of single data $(\\vec{x}, t)$ is\n",
    "    $$\n",
    "    \\begin{gathered}\n",
    "    t = \\vec{w}^T \\phi(\\vec{x}) + \\epsilon \\\\ \n",
    "    \\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n",
    "    \\end{gathered}\n",
    "    $$\n",
    "    of which $\\vec{w}^T \\phi(\\vec{x})$ is the true model, $\\epsilon$ is the perturbation/randomness.\n",
    "\n",
    "- Since $\\vec{w}^T \\phi(\\vec{x})$ is deterministic/non-random, we have\n",
    "    $$\n",
    "    t \\sim \\mathcal{N}(\\vec{w}^T \\phi(\\vec{x}), \\beta^{-1})\n",
    "    $$\n",
    "    \n",
    "**Exercise**: \n",
    "* Derive the likelihood function for a single data $p(t_n|\\vec{x}_n,\\vec{w},\\beta)$.\n",
    "* Derive the complete log likelihood function for the whole dataset $\\ln p(\\vec{t}|\\mathcal{X},\\vec{w},\\beta)$.\n",
    "* Using maximum likelihood to estimate parameter $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Maximum Likelihood Estimator $\\vec{w}_{ML}$\n",
    "    \n",
    "- The **likelihood function** of $t$ is just **probability density function (PDF)** of $t$\n",
    "    $$\n",
    "    p(t|\\vec{x},\\vec{w},\\beta) = \\mathcal{N}(t|\\vec{w}^T \\phi(\\vec{x}),\\beta^{-1})\n",
    "    $$\n",
    "    \n",
    "- For inputs $\\mathcal{X}=(\\vec{x}_1, \\dots, \\vec{x}_n)$ and target values $\\vec{t}=(t_1,\\dots,t_n)$, the data likelihood is\n",
    "    $$\n",
    "    p(\\vec{t}|\\mathcal{X},\\vec{w},\\beta)\n",
    "    = \\prod_{n=1}^N p(t_n|\\vec{x}_n,\\vec{w},\\beta)\n",
    "    = \\prod_{n=1}^N \\mathcal{N}(t_n|\\vec{w}^T\\phi(\\vec{x}_n),\\beta^{-1})\n",
    "    $$\n",
    "    \n",
    "- **Notation Clarification** \n",
    "    - $p(t|x,w,\\beta)$ it the PDF of $t$ whose distribution is parameterized by $x,\\vec{w},\\beta$. \n",
    "    - $\\mathcal{N}(\\vec{w}^T \\phi(\\vec{x}), \\beta^{-1})$ is Gaussian distribution with **mean** $\\vec{w}^T \\phi(\\vec{x})$ and **variance** $\\beta^{-1}$.\n",
    "    - $\\mathcal{N}(t|\\vec{w}^T \\phi(\\vec{x}),\\beta^{-1})$ is the PDF of $\\vec{t}$ which has Gaussian distribution $\\mathcal{N}(\\vec{w}^T \\phi(\\vec{x}), \\beta^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimator $\\vec{w}_{ML}$: Derivation\n",
    "- Single data likelihood is\n",
    "    $$\n",
    "    p(t_n|\\vec{x}_n,\\vec{w},\\beta) \n",
    "    = \\mathcal{N}(t_n|\\vec{w}^T\\phi(\\vec{x}_n),\\beta^{-1})\n",
    "    = \\frac{1}{\\sqrt{2 \\pi \\beta^{-1}}} \\exp \\left \\{ - \\frac{1}{2 \\beta^{-1}} (t_n - \\vec{w}^T \\phi(x_n))^2 \\right \\}\n",
    "    $$\n",
    "\n",
    "- Single data log-likelihood is \n",
    "    $$\n",
    "    \\ln p(t_n|\\vec{x}_n,\\vec{w},\\beta) = - \\frac12 \\ln 2 \\pi \\beta^{-1} - \\frac{\\beta}{2} (\\vec{w}^T \\phi(x_n) - t_n)^2\n",
    "    $$\n",
    "    We use logarithm because maximizer of $f(x)$ is the same as maximizer of $\\log f(x)$. Logarithm can convert product to summation which makes life easier.\n",
    "\n",
    "- Complete data log-likelohood is\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\ln p(\\vec{t}|\\mathcal{X},\\vec{w},\\beta) \n",
    "    &= \\ln \\left[ \\prod_{n=1}^N p(t_n|\\vec{x}_n,\\vec{w},\\beta) \\right] = \\sum_{n=1}^N \\ln p(t_n|\\vec{x}_n,\\vec{w},\\beta) \\\\\n",
    "    &= \\sum_{n=1}^N \\left[ - \\frac12 \\ln 2 \\pi \\beta^{-1} - \\frac{\\beta}{2} (\\vec{w}^T \\phi(x_n) - t_n)^2 \\right]\n",
    "    \\end{aligned}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Maximum likelihood estimate $\\vec{w}_{ML}$ is\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\vec{w}_{ML} \n",
    "    &= \\underset{\\vec{w}}{\\arg \\max} \\ln p(\\vec{t}|\\mathcal{X},\\vec{w},\\beta) \\\\\n",
    "    &= \\underset{\\vec{w}}{\\arg \\max}  \\sum_{n=1}^N \\left[ - \\frac12 \\ln 2 \\pi \\beta^{-1} - \\frac{\\beta}{2} (\\vec{w}^T \\phi(x_n) - t_n)^2 \\right] \\\\\n",
    "    &= \\underset{\\vec{w}}{\\arg \\max}  \\sum_{n=1}^N \\left[ - \\frac{\\beta}{2} (\\vec{w}^T \\phi(x_n) - t_n)^2 \\right] \\\\\n",
    "    &= \\underset{\\vec{w}}{\\arg \\min}  \\sum_{n=1}^N \\left[(\\vec{w}^T \\phi(x_n) - t_n)^2 \\right]\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "- Familiar? Recall the objective function we minimized in least squares is $E(\\vec{w}) = \\frac12 \\sum_{n=1}^N \\left( \\vec{w}^T \\phi(\\vec{x}_n) - t_n \\right)^2$, so we could conclude that\n",
    "    $$\n",
    "    \\boxed{\\vec{w}_{ML}  = \\hat{\\vec{w}}_{LS} = \\Phi^\\dagger \\vec{t}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MAP Estimator $\\vec{w}_{MAP}$\n",
    "- The **MAP estimator** is obtained by\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\vec{w}_{MAP} \n",
    "    &= \\arg \\max p(\\vec{w}|\\vec{t}, \\mathcal{X},\\beta) & & (\\text{Posteriori Probability})\\\\\n",
    "    &= \\arg \\max \\frac{p(\\vec{w}, \\vec{t}, \\mathcal{X},\\beta)}{p(\\mathcal{X}, t, \\beta)} \\\\\n",
    "    &= \\arg \\max \\frac{p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) p(\\vec{w}, \\mathcal{X}, \\beta)}{p(\\mathcal{X}, t, \\beta)} \\\\\n",
    "    &= \\arg \\max p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) p(\\vec{w}, \\mathcal{X}, \\beta) & & (p(X, t, \\beta) \\text{ is irrelevant to} \\ \\vec{w})\\\\\n",
    "    &= \\arg \\max p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) p(\\vec{w}) p(\\mathcal{X}) p(\\beta) & & (\\text{Independence}) \\\\\n",
    "    &= \\arg \\max p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) p(\\vec{w}) & & (\\text{Likelihood} \\times \\text{Prior})\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    We are just using **Bayes Theorem** for the above steps.\n",
    "- The only difference from ML estimator is we have an extra term of PDF of $\\vec{w}$. This is the **prior belief** of $\\vec{w}$. Here, we assume, \n",
    "    $$\n",
    "    \\vec{w} \\sim \\mathcal{N}(\\vec{0}, \\alpha^{-1}I) \n",
    "    $$\n",
    "    \n",
    "**Exercise**: Derive the MAP Estimator of $\\vec{w}$ and compare the solution with regularized linear regression. What is $\\lambda$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MAP Estimator $\\vec{w}_{MAP}$: Derivation\n",
    "- $\\vec{w} \\sim \\mathcal{N}(\\vec{0}, \\alpha^{-1}I)$ is **multivariate Gaussian** which has PDF\n",
    "    $$\n",
    "    p(\\vec{w}) = \\frac{1}{\\left( \\sqrt{2 \\pi \\alpha^{-1}} \\right)^N} \\exp \\left \\{ -\\frac{1}{2 \\alpha^{-1}} \\sum_{n=1}^N w_n^2 \\right \\}\n",
    "    $$\n",
    "\n",
    "- So the MAP estimator is \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "     \\vec{w}_{MAP} \n",
    "     &= \\underset{\\vec{w}}{\\arg \\max} \\ p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) p(\\vec{w}) = \\underset{\\vec{w}}{\\arg \\max} \\left[\\ln p(\\vec{t}|\\vec{w}, \\mathcal{X},\\beta) + \\ln p(\\vec{w}) \\right] \\\\\n",
    "     &= \\underset{\\vec{w}}{\\arg \\min} \\left[ \\sum_{n=1}^N \\frac{\\beta}{2} (\\vec{w}^T \\phi(x_n) - t_n)^2  +  \\frac{\\alpha}{2} \\sum_{n=1}^N w_n^2 \\right] \\\\\n",
    "     &= \\underset{\\vec{w}}{\\arg \\min} \\left[ \\sum_{n=1}^N \\frac12 (\\vec{w}^T \\phi(x_n) - t_n)^2  +  \\frac12 \\frac{\\alpha}{\\beta} \\left \\| \\vec{w} \\right \\|^2 \\right]\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "- Exactly the objective in regularized least squares! So\n",
    "$$\n",
    "\\boxed{ \\vec{w}_{MAP} = \\hat{\\vec{w}}=\\left(\\Phi^T \\Phi + \\frac{\\alpha}{\\beta} I\\right)^{-1} \\Phi^T \\vec{t} }\n",
    "$$\n",
    "\n",
    "- Compared with $\\ell^2$ norm regularized least square, we have $\\lambda = \\frac{\\alpha}{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 5a: MAP estimation for Linear Regression with unusual Prior\n",
    "Assume we have $n$ vectors $\\vec{x}_1, \\cdots, \\vec{x}_n$. We also assume that for each $\\vec{x}_i$ we have observed a *target* value $t_i$, where\n",
    "$$\n",
    "\\begin{gather}\n",
    "t_i = \\vec{w}^T \\vec{x_i} + \\epsilon \\\\ \n",
    "\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n",
    "\\end{gather}\n",
    "$$\n",
    "where $\\epsilon$ is the \"noise term\".\n",
    "\n",
    "(a) Quick quiz: what is the likelihood *given* $\\vec{w}$? That is, what's $p(t_i | \\vec{x}_i, \\vec{w})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer**: $p(t_i | \\vec{x}_i, \\vec{w}) = \\mathcal{N}(t_i|\\vec{w}^\\top \\vec{x_i}, \\beta^{-1}) = \\frac{1}{(2\\pi \\sigma^2)^\\frac{1}{2}} \\exp{(-\\frac{\\beta}{2}(t_i - \\vec{w}^\\top \\vec{x_i})^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 5: MAP estimation for Linear Regression with unusual Prior\n",
    "Assume we have $n$ vectors $\\vec{x}_1, \\cdots, \\vec{x}_n$. We also assume that for each $\\vec{x}_i$ we have observed a *target* value $t_i$, sampled IID. We will also put a *prior* on $\\vec{w}$, using PSD matrix $\\Sigma$.\n",
    "$$\n",
    "\\begin{gather}\n",
    "t_i = \\vec{w}^T \\vec{x_i} + \\epsilon \\\\ \n",
    "\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1}) \\\\\n",
    "\\vec{w} \\sim \\mathcal{N}(0, \\Sigma)\n",
    "\\end{gather}\n",
    "$$\n",
    "Note: the difference here is that our prior is a multivariate gaussian with *non-identity* covariance! Also we let $\\mathcal{X} = \\{\\vec{x}_1, \\cdots, \\vec{x}_n\\}$\n",
    "\n",
    "(a) Compute the log posterior function, $\\log p(\\vec{w}|\\vec{t}, \\mathcal{X},\\beta)$\n",
    "\n",
    "*Hint*: use Bayes' Rule\n",
    "\n",
    "(b) Compute the MAP estimate of $\\vec{w}$ for this model\n",
    "\n",
    "*Hint*: the solution is very similar to the MAP estimate for a gaussian prior with identity covariance"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
