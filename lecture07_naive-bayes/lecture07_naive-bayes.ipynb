{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|_2}\n",
    "\\newcommand{\\d}{\\mathop{}\\!\\mathrm{d}}\n",
    "\\newcommand{\\qed}{\\qquad \\mathbf{Q.E.D.}}\n",
    "\\newcommand{\\vx}{\\mathbf{x}}\n",
    "\\newcommand{\\vy}{\\mathbf{y}}\n",
    "\\newcommand{\\vt}{\\mathbf{t}}\n",
    "\\newcommand{\\vb}{\\mathbf{b}}\n",
    "\\newcommand{\\vw}{\\mathbf{w}}\n",
    "\\newcommand{\\vm}{\\mathbf{m}}\n",
    "\\newcommand{\\I}{\\mathbb{I}}\n",
    "\\newcommand{\\th}{\\text{th}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from Lec07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 445:  Introduction to Machine Learning\n",
    "## Lecture 07:  Naive Bayes\n",
    "\n",
    "* Instructor:  **Jacob Abernethy** and **Jia Deng**\n",
    "* Date:  September 28, 2016\n",
    "\n",
    "*Lecture Exposition Credit:*  Benjamin Bray, Valli Chockalingam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- Probabilistic Models\n",
    "    - Generative Models\n",
    "    - Discriminative Models\n",
    "- Naive Bayes Classifiers\n",
    "    - Independence Assumption\n",
    "    - MLE and MAP Parameter Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "- Suggested:\n",
    "    - **[PRML]**, §4.2: Probabilistic Generative Models\n",
    "    - **[PRML]**, §4.3: Probabilistic Discriminative Models\n",
    "    - **[MLAPP]**, §3.5: Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> In this lecture, we will first talk about the some concepts of probabilistic models for classifiers, especially generative model and discriminant model. And we will introduce Naive Bayes classifier, which assumes independent features give label and is a classical classifier commonly used in spam email classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probablistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probablistic Models: Generative Models\n",
    "\n",
    "- **Generative model** learns *class-conditional * $P(X | Y)$ and label densities $P(Y)$ from training data\n",
    "\n",
    "- Perform prediction using the **posterior** via Bayes' Rule. For some new data $\\vx^{new}$\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y \n",
    "    & = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg \\max} P(Y=k | X = \\vx^{new} ) \\\\\n",
    "    & = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg \\max} \\frac{P(X = \\vx^{new} | Y=k)P(Y=k)}{P(X = \\vx^{new})} \\\\\n",
    "    & = \\boxed{\\underset{k \\in \\{1, \\dots, K\\}}{\\arg \\max} P(X = \\vx^{new} | Y=k)P(Y=k)}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    of which the last equality holds because the denominator $P(X = \\vx^{new})$ is independent of $k$.\n",
    "\n",
    "- Basic idea of prediction is picking the label with largest posterior probability given its features $\\vx^{new}$.\n",
    "\n",
    "- Why is this model called **generative**?\n",
    "    - We learned *class-conditional probabiliy* $P(X | Y)$ from training data.\n",
    "    - $P(X | Y)$ is distribution of data $X$ given label $Y$\n",
    "    - So given some label $Y$, could **generate**/sample new data $X$ from $P(X | Y)$.\n",
    "\n",
    "- The *prior* $P(Y)$ encodes beliefs about popularity of each label\n",
    "\n",
    "- By comparing the synthetic data and real data, we get a sense of how good our generative model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probablistic Models: Discriminative Models\n",
    "\n",
    "- Conversely, a **discriminative model** learns posterior $P(Y | X)$ directly from training data.\n",
    "\n",
    "- Goal: select a hypothesis to *discriminates* between class labels.\n",
    "\n",
    "- The prediction for some new data $\\vx^{new}$ is\n",
    "    $$ y = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg \\max} P(Y=k | X = \\vx^{new} ) \\\\$$\n",
    "\n",
    "- Does not (necessarily) provide the ability to **generate** new random examples because unlike generative models, we have no idea what $P(X|Y)$ is.\n",
    "\n",
    "- Allows us to focus purely on the classification task\n",
    "\n",
    "- We will discuss the pros and cons of each model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probablistic Models: Discriminative Models—Property\n",
    "\n",
    "- The discriminative approach will typically \n",
    "    - make fewer generative assumptions about the data\n",
    "        - However, reconstruction features from labels may require prior knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifiers\n",
    "\n",
    "> Follows the approach taken by **[MLAPP]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{1, \\dots ,M \\}$, $\\forall d = 1, \\dots, D$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y       &\\sim \\mathrm{Categorical}(\\pi) \\\\\n",
    "    x_d | y=c &\\sim \\mathrm{Categorical}(\\theta_{cd}) \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "    with parameters:\n",
    "    - **Class priors** $\\pi = (\\pi_1, \\dots, \\pi_C) \\in \\Delta^C$, \n",
    "        - i.e. $P(y = c) = \\pi_c$, $\\forall c = 1,\\dots,C $\n",
    "        - $\\Delta^C$ is C-**simplex**. $\\pi \\in \\Delta^C$ is saying that $\\sum_{c=1}^C \\pi_c = 1$ and $\\pi_c \\geq 0, \\forall c=1,\\dots,C$\n",
    "    - **Class-conditional probabilities** $\\theta_{cd} = (\\theta_{cd1},  \\dots, \\theta_{cdM}) \\in \\Delta^M$\n",
    "        - i.e. $P(x_d = m| y = c) = \\theta_{cdm}$ for every $d = 1,\\dots,D, m = 1, \\dots, M, c = 1, \\dots, C$\n",
    "\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**Remark**\n",
    "> - **NOTE** in definition and derivation of this lecture, we assume a more general case $x_d \\in \\{1, \\dots ,M \\}$ of which $M>2$. But in spam email classification and the derivation in textbook, binary feature, i.e. $M=2$, is used. So don't get confused!\n",
    "\n",
    "> - When $M=2$, $x_d | y=c$ is also Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cdx_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- If we assume $x_d^{\\text{new}} \\in \\{1,\\dots,M \\}, \\forall d = 1,\\dots,D$, we could also express the above expression equivalently using **indicator function**\n",
    "    $$\n",
    "    y = \\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Indicator function\n",
    "    $$\n",
    "    \\mathbb{I}(m=x_d^{\\text{new}}) = \n",
    "    \\begin{cases}\n",
    "    1 & \\text{ if } m=x_d^{\\text{new}}\\\\ \n",
    "    0 & \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "> - In inner product $\\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}$, only $\\theta_{cdx_d^{\\text{new}}}$ is multiplied and all the other multipliers are 1 due to the power of indicator function.\n",
    "    \n",
    "> - One thing to note is that the above classification criterion is the product of a series numbers smaller than 1 which will generate a rather small number. A better way is to take **logarithm** to transform product into summation and then compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Parameter Estimation\n",
    "\n",
    "- **Goal:** Given training data $\\mathcal{D} = \\{ (\\vec{x}_1, y_1), \\dots, (\\vec{x}_N, y_N) \\}$, estimate **class-conditional probabilities** $\\theta$ and **class priors** $\\pi$.\n",
    "\n",
    "\n",
    "- We will discuss the **MLE** and **MAP** parameter estimates.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- The **likelihood** for a single data case $(\\vec{x}_n, y_n=c)$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= P(y_n) \\prod \\nolimits_{d=1}^D P(x_{nd}|y_n) \\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C P(y_n=c)^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M P(x_{nd}=m|y_n=c)^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C \\pi_c^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M \\theta_{cdm}^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- Therefore, the **log-likelihood** is\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    & \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    & = \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "- The **log-likelihood** for all training data $\\mathcal{D} = \\{ (\\vec{x}_n, y_n) \\}_{n=1}^N $ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & \\log P(\\mathcal{D}| \\pi, \\theta)\\\\\n",
    "    &= \\log \\prod \\nolimits_{n=1}^N P((\\vec{x}_n, y_n) | \\pi, \\theta) = \\sum \\nolimits_{n=1}^N \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= \\boxed{\\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}}\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- With the constraints $\\sum_{c=1}^C \\pi_c=1$ and $\\sum_{m=1}^M \\theta_{cdm}=1$, we could maximize log-likelihood function $\\log P(\\mathcal{D}| \\pi, \\theta)$ using *Lagrange multiplier*. (Derivation is in the notes!)\n",
    "\n",
    "- By maximizing log-likelihood function, we could have maximum likelihood estimators:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cdm} = \\frac{N_{cdm}}{N_c}\n",
    "    $$\n",
    "    and\n",
    "    $$\n",
    "    \\hat{\\pi} = (\\hat{\\pi}_1, \\dots,\\hat{\\pi}_c, \\dots,\\hat{\\pi}_C); \\hat{\\theta}_{cd} = (\\hat{\\theta}_{cd1}, \\dots,\\hat{\\theta}_{cdm}, \\dots,\\hat{\\theta}_{cdM})\n",
    "    $$\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cdm} = $ Number of examples in class $c$ with $x_d = m$ in $\\mathcal{D}$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{1,\\dots,M \\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Derivation of **maximum likelihood estimator** $\\hat{\\pi}_c$\n",
    "\n",
    ">     - We have the following problem\n",
    "        $$\n",
    "        \\begin{matrix}\n",
    "        \\left\\{\n",
    "        \\begin{split}\n",
    "        \\max \\quad &\\log P(\\mathcal{D}| \\pi, \\theta) \\\\\n",
    "        \\text{s.t.}  \\quad &\\sum \\nolimits_{c=1}^C \\pi_c=1\n",
    "        \\end{split} \\right.\n",
    "        &\n",
    "        \\overset{\\text{equivalent to}}{\\Longrightarrow}\n",
    "        &\n",
    "        \\left \\{\n",
    "        \\begin{split}\n",
    "        \\max \\quad & \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c \\\\\n",
    "        \\text{s.t.} \\quad &\\sum \\nolimits_{c=1}^C \\pi_c = 1\n",
    "        \\end{split}\n",
    "        \\right.\n",
    "        \\end{matrix}\n",
    "        $$\n",
    "        We drop the second term in $\\log P(\\mathcal{D}| \\pi, \\theta)$ because it doesn't depend on $\\pi_c$\n",
    ">     - The lagragian is\n",
    "        $$\n",
    "        L(\\pi, \\lambda) = \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c - \\lambda \\sum \\nolimits_{c=1}^C \\pi_c -\\lambda\n",
    "        $$\n",
    ">     - Setting partial derivative with respect to $\\pi_c$ to 0, we have\n",
    "        $$\n",
    "        \\frac{\\partial L(\\pi, \\lambda)}{\\partial \\pi_c} = 0\n",
    "        \\quad \\Rightarrow  \\quad \n",
    "        \\sum \\nolimits_{n=1}^N \\I(y_n=c) \\frac{1}{\\pi_c} - \\lambda = 0  \n",
    "        \\quad \\Rightarrow  \\quad \n",
    "        \\pi_c = \\frac{1}{\\lambda} \\sum \\nolimits_{n=1}^N \\I(y_n=c)\n",
    "        $$\n",
    ">     - Plug $\\pi_c$ back into the constraint $\\sum \\nolimits_{c=1}^C \\pi_c=1$, we have\n",
    "        $$\n",
    "        \\frac{1}{\\lambda} \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{n=1}^N \\I(y_n=c) = 1\n",
    "        \\quad \\Rightarrow  \\quad \n",
    "        \\lambda = \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{n=1}^N \\I(y_n=c)\n",
    "        $$\n",
    ">     - Plug $\\lambda$ into $\\pi_c = \\frac{1}{\\lambda} \\sum \\nolimits_{n=1}^N \\I(y_n=c)$, we have\n",
    "        $$\n",
    "        \\hat{\\pi}_c = \\frac{\\sum \\nolimits_{n=1}^N \\I(y_n=c)}{ \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c)} = \\frac{N_c}{N}\n",
    "        $$\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> - Derivation of maximum likelihood estimator $\\hat{\\theta}_{cdm}$\n",
    "\n",
    ">     - With the constraint $\\sum_{m=1}^M \\theta_{cdm}=1$, using similar approach, we could have \n",
    "        $$\n",
    "        \\hat{\\theta}_{cdm}\n",
    "        = \\frac{\\sum \\nolimits_{n=1}^N \\I(x_{nd}=m) \\I(y_n=c)}{\\sum_{n=1}^N \\sum_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c)}\n",
    "        = \\frac{N_{cdm}}{N_c}\n",
    "        $$\n",
    "        \n",
    ">    - Details are left as an exercise XD     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Sparse Features\n",
    "\n",
    "- **Problem:** When working with text, features are **sparse**:\n",
    "    - In training, we only see a *small, small* fraction of words in the vocabulary\n",
    "    - Moreover, we won't see all words exhibited across all classes\n",
    "\n",
    "- This causes overfitting!\n",
    "    - What if a word (e.g. \"`subject:`\") occurs in every training example of both classes?\n",
    "    - Then if we encounter a new email without this word, our algorithm will crash.\n",
    "    - What happens if that word never appears in testing?  (*Black Swan Paradox*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Priors\n",
    "\n",
    "- **Solution:** Place Dirichlet priors on $\\pi$ and $\\theta_{cd}$ to *smooth out* unknowns:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\pi       &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_C) \\\\\n",
    "    \\theta_{cd}  &\\sim \\mathrm{Dirichlet}(\\beta_{cd1}, \\dots, \\beta_{cdM})  & &\\quad  \\forall\\, c=1,\\dots,C; d=1, \\dots D \\\\\n",
    "    y         &\\sim \\mathrm{Categorical}(\\pi)                     \\\\\n",
    "    x_d | y=c &\\sim \\mathrm{Categorical}(\\theta_{cd}) & & \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- **Dirichlet distribution** $\\pi \\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_C)$ defines the distribution of C-simplex $\\pi = (\\pi_1, \\dots, \\pi_C)$ such that\n",
    "    - $\\pi_1, \\dots, \\pi_C \\geq 0$\n",
    "    - $\\pi_1+ \\dots + \\pi_C = 1$\n",
    "    - PDF $f(\\pi_1, \\dots, \\pi_C ) = \\frac{1}{B(\\alpha)} \\prod_{c=1}^C \\pi_c^{\\alpha_c-1}$, where the quantity $B(\\alpha)$ is to normalize the distribution\n",
    "\n",
    "- When $M=2$, $\\theta_{cd}$ reduces to **Beta** distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  MAP Estimate\n",
    "\n",
    "- The **MAP parameter** estimates with priors\n",
    "    $$\n",
    "    \\pi \\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_C) \\qquad \\theta_{cd} \\sim \\mathrm{Dirichlet}(\\beta_{cd1}, \\dots, \\beta_{cdM})\n",
    "    $$\n",
    "    are\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c+\\alpha_c-1}{N + \\sum_{c'=1}^C (\\alpha_{c'}-1)} \n",
    "    \\quad \n",
    "    \\hat{\\theta}_{cdm} = \\frac{N_{cdm}+\\beta_{cdm}-1}{N_c + \\sum_{m'=1}^M (\\beta_{cdm'}-1)}\n",
    "    $$\n",
    "    \n",
    "- Proof is in the notes!    \n",
    "\n",
    "- The Dirichlet $\\alpha$ and $\\beta_{cd}$ parameters turn out to be **pseudocounts**!  \n",
    "    - We assume we've seen $\\alpha_c$ examples of class $c$ beforehand\n",
    "    - and $\\beta_{cdm}$ examples with $x_d = m$ in class $c$.\n",
    "\n",
    "- The choice $\\alpha_c = \\beta_{cdm} = 1$ is referred to as **Laplace Smoothing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Mean Estimate\n",
    "\n",
    "- Note that the posterior of parameters still have **Dirichlet** distributions!\n",
    "    $$\n",
    "    \\pi|\\mathcal{D} \\sim \\mathrm{Dirichlet}(N_1+\\alpha_1, \\dots, N_C+\\alpha_C) \\qquad \\theta_{cd}|\\mathcal{D} \\sim \\mathrm{Dirichlet}(N_{cd1}+\\beta_{cd1}, \\dots, N_{cdM}+\\beta_{cdM})\n",
    "    $$\n",
    "    Proof for $\\pi|\\mathcal{D}$ is in the notes! Proof for $\\theta_{cd}|\\mathcal{D}$ is left as an exercise.\n",
    "    \n",
    "- If we pick the **mean** of posteriors as parameter estimate, we could get a slightly different result:\n",
    "    $$\n",
    "    \\bar{\\pi}_c = \\frac{N_c+\\alpha_c}{N + \\sum_{c'=1}^C \\alpha_{c'}} \n",
    "    \\quad \n",
    "    \\bar{\\theta}_{cdm} = \\frac{N_{cdm}+\\beta_{cdm}}{N_c + \\sum_{m'=1}^M \\beta_{cdm'}}\n",
    "    $$\n",
    "    $-1$ terms no longer exist!\n",
    "    \n",
    "- You might see this version of estimate in **[MLAPP]** and some online materials\n",
    "\n",
    "- Advantage of using mean estimate\n",
    "    - Since posteriors still have Dirichlet distribution, we could use posterior as the prior for the next learning phase!\n",
    "    - This can be helpful in sequential learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Posterior also has Dirichlet distribution!\n",
    "\n",
    ">     - **Prior:**\n",
    "        $$\n",
    "        \\pi \\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_C) \n",
    "        \\qquad\n",
    "        f(\\pi) = \\frac{1}{B(\\alpha)} \\prod \\nolimits_{c=1}^C \\pi_c^{\\alpha_c-1}\n",
    "        $$\n",
    "\n",
    ">     - **Likelihood:**\n",
    "        $$\n",
    "        P(\\mathcal{D} | \\pi) = \\prod \\nolimits_{n=1}^N \\prod \\nolimits_{c=1}^C \\pi_c^{\\I(y_n=c)} = \\prod \\nolimits_{c=1}^C \\pi_c^{\\sum \\nolimits_{n=1}^N \\I(y_n=c)} = \\prod \\nolimits_{c=1}^C \\pi_c^{N_c}\n",
    "        $$\n",
    "\n",
    ">     - **Posterior**\n",
    "        $$\n",
    "        \\begin{split}\n",
    "        f(\\pi | \\mathcal{D}) \n",
    "        &= \\frac{P(\\mathcal{D} | \\pi) f(\\pi)}{P(\\mathcal{D})} = \\frac{1}{P(\\mathcal{D})} \\prod \\nolimits_{c=1}^C \\pi_c^{N_c} \\cdot \\frac{1}{B(\\alpha)} \\prod \\nolimits_{c=1}^C \\pi_c^{\\alpha_c-1} \\\\\n",
    "        &= \\frac{1}{P(\\mathcal{D}) B(\\alpha)} \\prod \\nolimits_{c=1}^C \\pi_c^{N_c+\\alpha_c-1} \\\\\n",
    "        &= \\frac{1}{B'(\\alpha)} \\prod \\nolimits_{c=1}^C \\pi_c^{N_c+\\alpha_c-1}\n",
    "        \\end{split}\n",
    "        $$\n",
    "        of which $B'(\\alpha) = P(\\mathcal{D}) B(\\alpha)$.\n",
    "        \n",
    ">     - From the expression of $f(\\pi | \\mathcal{D}) $, we could see posterior also has **Dirichlet** distribution\n",
    "        $$\n",
    "        \\pi | \\mathcal{D} \\sim \\mathrm{Dirichlet}(N_1+\\alpha_1, \\dots, N_C+\\alpha_C)\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> - Conjugate Prior\n",
    "\n",
    ">     - If posterior and prior are in the same distribution family with respect to some likelihood, we could call this distribution **conjugate prior** for that likelihood.\n",
    "\n",
    ">     - This is useful because we could take the posterior as the prior for next learning phase, which enables us to do sequential Bayesian learning.\n",
    "\n",
    ">     - In our case, we have shown Dirichlet distribution is the conjugate prior of the multinomial distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> - Derivation of **MAP estimate** ${\\hat{\\pi}_c}_{MAP}$\n",
    "\n",
    ">     - MAP estimate is obtained by maximizing the posterior $f(\\pi | \\mathcal{D})$\n",
    "        $$\n",
    "        \\begin{matrix}\n",
    "        \\left\\{\n",
    "        \\begin{split}\n",
    "        \\max \\quad & \\prod \\nolimits_{c=1}^C \\pi_c^{N_c+\\alpha_c-1}\\\\\n",
    "        \\text{s.t.}  \\quad &\\sum \\nolimits_{c=1}^C \\pi_c=1\n",
    "        \\end{split} \\right.\n",
    "        &\n",
    "        \\overset{\\text{equivalent to}}{\\Longrightarrow}\n",
    "        &\n",
    "        \\left \\{\n",
    "        \\begin{split}\n",
    "        \\max \\quad & \\sum \\nolimits_{c=1}^C \\left( N_c + \\alpha_c -1 \\right) \\log \\pi_c \\\\\n",
    "        \\text{s.t.} \\quad &\\sum \\nolimits_{c=1}^C \\pi_c = 1\n",
    "        \\end{split}\n",
    "        \\right.\n",
    "        \\end{matrix}\n",
    "        $$\n",
    "\n",
    ">     - Recall when deriving maximum likelihood estimator, we solved the following problem\n",
    "        $$\n",
    "        \\left \\{\n",
    "        \\begin{split}\n",
    "        \\max \\quad & \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{n=1}^N \\I(y_n=c) \\log \\pi_c \\\\\n",
    "        \\text{s.t.} \\quad &\\sum \\nolimits_{c=1}^C \\pi_c = 1\n",
    "        \\end{split}\n",
    "        \\right.\n",
    "        \\longrightarrow\n",
    "        {\\hat{\\pi}_c}_{MLE} = \\frac{\\sum \\nolimits_{n=1}^N \\I(y_n=c)}{ \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c)} = \\frac{N_c}{N}\n",
    "        $$\n",
    "        \n",
    ">     - So the solution of our current problem can be easily read off\n",
    "        $$\n",
    "        \\begin{split}\n",
    "        {\\hat{\\pi}_c}_{MAP} \n",
    "        &= \\frac{N_c + \\alpha_c -1 }{\\sum \\nolimits_{c'=1}^C \\left(N_c + \\alpha_c' -1 \\right)} = \\frac{N_c + \\alpha_c -1}{N + \\sum \\nolimits_{c'=1}^C \\left(\\alpha_c' -1\\right)}\n",
    "        \\end{split}\n",
    "        $$\n",
    "\n",
    "> - Derivation of **MAP estimator** $\\hat{\\theta}_{cdm}$ is left as an exercise! Approach is exactly the same as deriving $\\hat{\\pi}_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Is Independence Justified?\n",
    "\n",
    "- Naive Bayes assumes features contribute *independently* to the class label.\n",
    "    \n",
    "    - This is the *simplest possible* generative model... and an **extreme** assumption...\n",
    "\n",
    "- This model is *naive* because we would never expect features to be independent!\n",
    "\n",
    "    -  We are completely ignoring correlations between variables!\n",
    "\n",
    "- It seems not to matter that independence is often false...\n",
    "    - Naive Bayes performs surprisingly well on real-world data\n",
    "    - Naive Bayes is often used as a baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Classifiers\n",
    "- **Logistic Regression**\n",
    "    * Provides model for $P(y | \\vx)$ using sigmoid\n",
    "    * No explicit model for $P(\\vx | y)$\n",
    "- **Naive Bayes**\n",
    "    * Provides a full model for $P(\\vx | y )$ and $P(y)$\n",
    "    * Assumes independence between features *conditioned on* target $y$\n",
    "    * Typically requires discrete data (can generalize to continuous spaces)\n",
    "    * ML estimates are pretty straightforward\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
