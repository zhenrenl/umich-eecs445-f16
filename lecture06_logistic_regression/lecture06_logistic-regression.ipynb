{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|_2}\n",
    "\\newcommand{\\d}{\\mathop{}\\!\\mathrm{d}}\n",
    "\\newcommand{\\qed}{\\qquad \\mathbf{Q.E.D.}}\n",
    "\\newcommand{\\vx}{\\mathbf{x}}\n",
    "\\newcommand{\\vy}{\\mathbf{y}}\n",
    "\\newcommand{\\vt}{\\mathbf{t}}\n",
    "\\newcommand{\\vb}{\\mathbf{b}}\n",
    "\\newcommand{\\vw}{\\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'Lec07'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8acfaa2cb410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mLec07\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'Lec07'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Lec07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 445: Introduction to Machine Learning\n",
    "## Lecture 06:  Logistic Regression \n",
    "* Instructor:  **Jacob Abernethy** and **Jia Deng**\n",
    "* Date:  September 26, 2016\n",
    "\n",
    "*Lecture Exposition Credit:*  Benjamin Bray, Saket Dewangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "- Concept of Classification\n",
    "- Logistic Regression\n",
    "    - Intuition, Motivation\n",
    "    - Newton's Method\n",
    "    \n",
    "## Reading List\n",
    "\n",
    "- Suggested:\n",
    "    - **[MLAPP]**, Chapter 8: Logistic Regression    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> In this lecture, we will move from regression to classification.Unlike of predicting some value for data in regression, we predict what category data belongs to in classification. And we will introduce logistic regression. In logistic regression, we will show how to find the optimal coefficients $\\vw$ using Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Supervised Learning\n",
    "\n",
    "- Goal\n",
    "    - Given data $X$ in feature sapce and the labels $Y$\n",
    "    - Learn to predict $Y$ from $X$\n",
    "- Labels could be discrete or continuous\n",
    "    - Discrete-valued labels:  Classification\n",
    "    - Continuous-valued labels:  Regression\n",
    "    \n",
    "<center> <img src=\"images/classification-regression.png\"  style=\"width:473px;height:213px;\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Problem: Basics\n",
    "\n",
    "- Given an input vector $\\vx$, assign it to one of $K$ distinct classes $C_k$, where $k = 1,\\dots,K$.\n",
    "\n",
    "- The case $K=2$ is **Binary Classification**\n",
    "    - Label $t=1$ means $x \\in C_1$\n",
    "    - Label $t=0$ means $x \\in C_2$ (or sometimes $t=-1$)\n",
    "    \n",
    "- **Training:**  Learn a classifier $y(\\vx)$ from data,\n",
    "    $$ \\text{Training Data} \\quad \\{ (\\vx_1, t_1), \\dots, (\\vx_N, t_N) \\} \\implies \\text{Classifier} \\ y(\\vx) $$\n",
    "    \n",
    "- **Prediction:** Predict labels of new data,\n",
    "    $$ \\text{New Data} \\quad \\{ (\\vx^{new}_1, t^{new}_1), \\dots, (\\vx^{new}_m, t^{new}_m) \\} \\stackrel{h}{\\implies} \\{ y(\\vx^{new}_1), \\dots, y(x^{new}_m) \\} $$\n",
    "\n",
    "- **Performance Evaluation:** Evaluate learned classifier on test data,\n",
    "    $$ \\text{Test Data} \\quad \\{ (\\vx^{test}_1, t^{test}_1), \\dots, (\\vx^{test}_m, t^{test}_m) \\} \\stackrel{y}{\\implies} \\{ y(\\vx^{test}_1), \\dots, y(\\vx^{test}_m) \\} \\implies \\text{Error Estimate} $$\n",
    "    - To estimate **classification error**, we could use e.g. *zero-one loss*:\n",
    "        $$\n",
    "        E = \\frac{1}{m} \\sum_{j=1}^m \\mathbb{1} [ y(\\vx^{test}_j) \\neq t^{test}_j) ]\n",
    "        $$\n",
    "        i.e. number of misclassified data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Problems:  Strategies\n",
    "\n",
    "- **Nearest-Neighbors:**  Given query data $\\vx$, find closest training points and do a majority vote.\n",
    "\n",
    "- **Discriminant Functions:**  Learn a function $y(\\vx)$ mapping $\\vx$ to some class $C_k$.\n",
    "\n",
    "- **Probabilistic Model:**  Learn the distributions $P(C_k | \\vx)$\n",
    "    - *Discriminative Models* directly model $P(C_k | \\vx)$ and learn parameters from the training set.\n",
    "    - *Generative Models* learn class-conditional densities $P(\\vx | C_k)$ and priors $P(C_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "> - Logistic Regression is a technique for **classification**!\n",
    "\n",
    "> - We will focus on *binary* classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Preliminaryâ€”Logistic Sigmoid Function\n",
    "\n",
    "- The **logistic sigmoid function** is \n",
    "    $$\n",
    "    \\sigma(a)\n",
    "    = \\frac{1}{1 + \\exp(-a)}\n",
    "    = \\frac{\\exp(a)}{1 + \\exp(a)}\n",
    "    $$\n",
    "\n",
    "- Sigmoid function $\\sigma(a)$ maps $(-\\infty, +\\infty) \\to (0,1)$\n",
    "<center> <img src=\"images/sigmoidfunc.png\"  style=\"width:375px;height:256px;\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression:  Why use Logistic Sigmoid Function?\n",
    "- Prediction is picking the larger one of $P(y=1 | \\vx)$ and $P(y=0 | \\vx)$.\n",
    "\n",
    "- This can be implemented by evaluating **log odds**\n",
    "    $$\n",
    "    a = \\ln \\frac{P(y=1 | \\vx)}{P(y=0  | \\vx)}\n",
    "    $$\n",
    "    \n",
    "- So the prediction is\n",
    "    $$\n",
    "    y = \n",
    "    \\left\\{\\begin{matrix}\n",
    "    1& a\\geq 0\\\\\n",
    "    0& a< 0\n",
    "    \\end{matrix}\\right.\n",
    "    $$    \n",
    "    \n",
    "- Since $P(y=1 | \\vx) + P(y=0  | \\vx)=1$, we could solve for\n",
    "    $$\n",
    "    P(y=1 | \\vx) = \\frac{\\exp(a)}{1+\\exp(a)} = \\sigma(a)\n",
    "    $$ \n",
    "    *Logistic Function* appears!   \n",
    "\n",
    "- A heuristic choice for log odds is a separating **hyper plane** $a = \\vw^T \\phi(\\vx)$. So the criterion becomes\n",
    "    $$\n",
    "    \\boxed{\n",
    "    y = \n",
    "    \\left\\{\\begin{matrix}\n",
    "    1& \\vw^T \\phi(\\vx)\\geq 0, \\quad \\text{i.e.} \\quad \\sigma(\\vw^T\\phi(\\vx)\\geq 0) \\geq 0.5\\\\\n",
    "    0& \\vw^T \\phi(\\vx)< 0 , \\quad \\text{i.e.} \\quad \\sigma(\\vw^T\\phi(\\vx)\\geq 0) < 0.5\n",
    "    \\end{matrix}\\right.}\n",
    "    $$\n",
    "    \n",
    "- In this case, $P(y=1 | \\vx) = \\sigma(\\vw^T \\phi(\\vx))$ and $P(y=0 | \\vx) = 1-\\sigma(\\vw^T \\phi(\\vx))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Underlying Model\n",
    "\n",
    "- We already have\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    P(y=1 | \\vx, \\vw) &= \\sigma(\\vw^T \\phi(\\vx)) \\\\\n",
    "    P(y=0 | \\vx, \\vw) &= 1-\\sigma(\\vw^T \\phi(\\vx))\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- So we could model **class posterior** using Bernoulli random variable\n",
    "    $$\n",
    "    y | \\vx ,\\vw \\sim \\mathrm{Bernoulli}( \\sigma(\\vw^T \\phi(\\vx)) )\n",
    "    $$\n",
    "\n",
    "- We can obtain the best paramter $\\vw$ by maximizing the likelihood of the training data.(Later)\n",
    "\n",
    "- Logistic regression is simpest discriminative model that is **linear**\n",
    "in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6)); plot_linear_boundary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We could clearly see the linear boundary corresponding to $\\vw^T \\phi(\\vx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression:  Likelihood\n",
    "\n",
    "- We saw before that the **likelihood** for each binary label is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    P(y = 1 | \\vx,\\vw) &= \\sigma(\\vw^T \\phi(\\vx)) \\\\\n",
    "    P(y = 0 | \\vx,\\vw) &= 1 - \\sigma(\\vw^T \\phi(\\vx))\n",
    "    \\end{align}\n",
    "    $$\n",
    "- With a clever trick, this\n",
    "    $$\n",
    "    P(y | x,w) = \\sigma(\\vw^T \\phi(\\vx))^y \\cdot (1 - \\sigma(\\vw^T \\phi(\\vx)))^{1-y}\n",
    "    $$\n",
    "\n",
    "- For a data set $\\{(\\vx_n, t_n) \\}_{n=1}^N$ where $t_n \\in \\{ 0,1 \\}$, the **likelihood function** is\n",
    "    $$\n",
    "    P(\\vy = \\vt| \\mathcal{X}, \\vw) = \\prod_{n=1}^N P(y=t_n | \\vx_n, \\vw) =\\prod_{n=1}^N \\sigma(\\vw^T \\phi(\\vx_n))^{t_n} [1 - \\sigma(\\vw^T \\phi(\\vx_n))] ^{1-t_n}\n",
    "    $$\n",
    "    - where $\\mathcal{X} = \\{\\vx_n \\}_{n=1}^N$\n",
    "\n",
    "- The optimal $\\vw$ can be obtained by maximizing this likelihood.\n",
    "\n",
    "- Maximum likelihood estimate $\\vw_{ML}$ makse sense because $\\vw_{ML}$ is the coefficient that are most likely to produce $\\{t_n \\}_{n=1}^N$ given $\\mathcal{X}$.\n",
    "\n",
    "- Define **negative log-likelihood** as the **loss**\n",
    "    $$\n",
    "    E(\\vw) \\triangleq -\\ln P(\\vy = \\vt| \\mathcal{X}, \\vw)\n",
    "    $$\n",
    "- Maximizing **likelihood** is equivalent to minimizing **loss** $E(\\vw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression:  Gradient of Loss\n",
    "\n",
    "- Loss function $E(\\vw)$ can be transformed:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    E(\\vw) \n",
    "    &= -\\ln P(\\vy = \\vt| \\mathcal{X}, \\vw) \\\\\n",
    "    &= -\\ln \\prod \\nolimits_{n=1}^N \\sigma(\\vw^T \\phi(\\vx_n))^{t_n} [1 - \\sigma(\\vw^T \\phi(\\vx_n))] ^{1-t_n} \\\\\n",
    "    &= -\\sum \\nolimits_{n=1}^N \\left[ t_n \\ln \\sigma(\\vw^T \\phi(\\vx_n)) + (1-t_n) \\ln(1-\\sigma(\\vw^T \\phi(\\vx_n))) \\right] \\\\\n",
    "    &= -\\sum \\nolimits_{n=1}^N \\left[ t_n \\ln \\frac{\\exp(\\vw^T\\phi(\\vx_n))}{1+\\exp(\\vw^T\\phi(\\vx_n))} + (1-t_n) \\ln(\\frac{1}{1+\\exp(\\vw^T\\phi(\\vx_n))}) \\right] \\\\\n",
    "    &= -\\sum \\nolimits_{n=1}^N \\left[ t_n \\ln \\frac{1}{1+\\exp(-\\vw^T\\phi(\\vx_n))} + (1-t_n) \\ln(\\frac{1}{1+\\exp(\\vw^T\\phi(\\vx_n))}) \\right] \\\\\n",
    "    &= \\boxed{\\sum \\nolimits_{n=1}^N \\left[ t_n \\ln (1+\\exp(-\\vw^T\\phi(\\vx_n))) + (1-t_n) \\ln(1+\\exp(\\vw^T\\phi(\\vx_n))) \\right] }\\\\\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Gradient of loss $\\nabla_\\vw E(\\vw)$\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\nabla_\\vw E(\\vw)\n",
    "    &= \\nabla_\\vw \\sum \\nolimits_{n=1}^N \\left[ t_n \\ln (1+\\exp(-\\vw^T\\phi(\\vx_n))) + (1-t_n) \\ln(1+\\exp(\\vw^T\\phi(\\vx_n))) \\right] \\\\\n",
    "    &= \\sum \\nolimits_{n=1}^N \\left[-t_n \\frac{\\exp(-\\vw^T\\phi(\\vx_n))}{1+\\exp(-\\vw^T\\phi(\\vx_n))} \\phi(\\vx_n)+ (1-t_n) \\frac{\\exp(\\vw^T\\phi(\\vx_n))}{1+\\exp(\\vw^T\\phi(\\vx_n))} \\phi(\\vx_n) \\right] \\\\\n",
    "    &= \\sum \\nolimits_{n=1}^N \\left[-t_n (1-\\sigma(\\vw^T\\phi(\\vx_n)))+ (1-t_n) \\sigma(\\vw^T\\phi(\\vx_n)) \\right] \\phi(\\vx_n) \\\\\n",
    "    &= \\sum \\nolimits_{n=1}^N \\left[\\sigma(\\vw^T\\phi(\\vx_n)) - t_n \\right] \\phi(\\vx_n) \\\\\n",
    "    &= \\boxed{ \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right) }\n",
    "    \\end{align}\n",
    "    $$\n",
    "    of which\n",
    "    $$\n",
    "    \\Phi = \\begin{bmatrix} - & \\phi(\\vx_1)^T & -\\\\  & \\vdots & \\\\  - & \\phi(\\vx_N)^T & - \\end{bmatrix}_{N \\times M}\n",
    "    \\qquad\n",
    "    \\sigma(\\Phi \\vw)=\\begin{bmatrix}\n",
    "    \\sigma(\\vw^T\\phi(\\vx_1))\\\\ \n",
    "    \\vdots\\\\\n",
    "    \\sigma(\\vw^T\\phi(\\vx_N))\n",
    "    \\end{bmatrix}_{N \\times 1}\n",
    "    \\qquad\n",
    "    \\vt = \\begin{bmatrix}\n",
    "    t_1\\\\ \n",
    "    \\vdots\\\\\n",
    "    t_N\n",
    "    \\end{bmatrix}_{N \\times 1}\n",
    "    $$   \n",
    "    \n",
    "- With the gradient of loss, we could perform *gradient descent* to find $\\vw_{ML}$.\n",
    "\n",
    "- But we will use a new method by finding roots of first order derivative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> Remark\n",
    "> - Note that this gradient resembles the gradient in linear regression with least squares (Check Lecture 4)\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\text{Logistic Regression} \\quad & \\nabla_\\vw E(\\vw) = \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right) \\\\\n",
    "    \\text{Linear Regression} \\quad & \\nabla_\\vw E(\\vw) = \\Phi^T \\left( \\Phi \\vw - \\vt \\right)\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method:  Overview\n",
    "\n",
    "- First let's consider one dimension case.\n",
    "\n",
    "- **Goal:** Finding *root* of a general function $f(x)$, i.e. solve for $x$ such that $$f(x)=0$$\n",
    "\n",
    "- **Newton's Method:** Repeat until convergence:\n",
    "    $$\n",
    "    x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method:  Geometric Intuition\n",
    "\n",
    "- Find the roots of $f(x)$ by following its **tangent lines**. The tangent line of $f(x)$ at $x_n$ has equation\n",
    "    $$\n",
    "    \\ell(x) = f(x_n) + (x-x_n) f'(x_n)\n",
    "    $$\n",
    "- Set next iterate $x_{n+1}$ to be **root** of tangent line:\n",
    "    $$\n",
    "    \\begin{gather}\n",
    "    f(x_n) + (x-x_n) f'(x_n) = 0 \\\\\n",
    "    \\implies \\boxed{ x_{n+1}= x_n - \\frac{f(x_n)}{f'(x_n)} }\n",
    "    \\end{gather}\n",
    "    $$\n",
    "    \n",
    "<center> <img src=\"images/NewtonMethod.png\"  style=\"width:418px;height:240px;\"> </center>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fn(x): \n",
    "    return (x-2)**3; \n",
    "def d1(x): \n",
    "    return 3*(x-2)**2; \n",
    "newton_example(fn,d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> Remark\n",
    "> - Here is how Newton's method works\n",
    ">     - Given some initial point $x_1$, we first find the tangent line $\\ell_1(x)$ of $f(x)$ at $x_1$.\n",
    ">     - Let $x_2$ denote the root of $\\ell_1(x)$, i.e. $\\ell_1(x_2)=0$\n",
    ">     - Find the tangent line $\\ell_2(x)$ of $f(x)$ at $x_2$.\n",
    ">     - Let $x_3$ denote the root of $\\ell_2(x)$, i.e. $\\ell_1(x_3)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method: Finding Stationary Point\n",
    "\n",
    "- We have shown how to use Newton's method to find the root for $f(x)$\n",
    "    $$\n",
    "    x_{n+1}= x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "    $$\n",
    "\n",
    "- **Note** that Stationary point of $f(x)$ is equivalent to root of $f'(x)$\n",
    "\n",
    "- So, we could find stationary point of $f(x)$ by finding root of $f'(x)$ using Newton's method.\n",
    "\n",
    "- The iteration steps becomes\n",
    "    $$\n",
    "    x_{n+1}= x_n - \\frac{f'(x_n)}{f''(x_n)}\n",
    "    $$\n",
    "    \n",
    "- For *multi-dimension* case, this iteration turns into\n",
    "    $$\n",
    "    \\vx_{n+1}= \\vx_n - \\left(\\nabla^2 f(\\vx_n)\\right)^{-1} \\nabla_\\vx f(\\vx_n)\n",
    "    $$\n",
    "    of which $\\nabla^2 f(\\vx_n)$ is **Hessian matrix** which is the *second order derivative*\n",
    "    $$\n",
    "    \\nabla^2 f = \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1\\partial x_1} & \\cdots & \\frac{\\partial f}{\\partial x_1\\partial x_n}\\\\ \n",
    "    \\vdots & \\ddots & \\vdots\\\\ \n",
    "    \\frac{\\partial f}{\\partial x_n\\partial x_1} & \\cdots & \\frac{\\partial f}{\\partial x_n\\partial x_n}\n",
    "    \\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Applying Newton's Method\n",
    "\n",
    "- Back to logistic regression!\n",
    "\n",
    "- Recall our goal to minimize $E(\\vw)$ and we already have its gradient\n",
    "    $$\n",
    "    \\nabla_\\vw E(\\vw) = \\sum \\nolimits_{n=1}^N \\left[\\sigma(\\vw^T\\phi(\\vx_n)) - t_n \\right] \\phi(\\vx_n) = \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right)\n",
    "    $$\n",
    "    \n",
    "- To minimize of $E(\\vw)$, we could use Newton's method to find its *stationary point*!\n",
    "\n",
    "- To use Newton's method, we need the *Hessian matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Hessian Matrix\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla^2 E(\\vw) \n",
    "&= \\nabla_\\vw \\nabla_\\vw E(\\vw) \\\\\n",
    "&= \\nabla_\\vw \\sum \\nolimits_{n=1}^N \\left[\\sigma(\\vw^T\\phi(\\vx_n)) - t_n \\right] \\phi(\\vx_n) \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\nabla_\\vw \\sigma(\\vw^T\\phi(\\vx_n))  \\phi(\\vx_n) \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\nabla_\\vw  \\frac{1}{1 + \\exp(-\\vw^T \\phi(\\vx_n))}  \\phi(\\vx_n) \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) \\frac{\\exp(-\\vw^T \\phi(\\vx_n))}{(1 + \\exp(-\\vw^T \\phi(\\vx_n)))^2}  \\phi(\\vx_n)^T \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) \\frac{1}{1 + \\exp(-\\vw^T \\phi(\\vx_n))} \\frac{\\exp(-\\vw^T \\phi(\\vx_n))}{1 + \\exp(-\\vw^T \\phi(\\vx_n))}   \\phi(\\vx_n)^T \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) [\\sigma(\\vw^T \\phi(\\vx_n)) \\cdot ( 1 - \\sigma(\\vw^T \\phi(\\vx_n)) )]  \\phi(\\vx_n)^T \\\\\n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) r_n(\\vw) \\phi(\\vx_n)^T\n",
    "\\end{align}\n",
    "$$\n",
    "- of which $r_n(\\vw) = \\sigma(\\vw^T \\phi(\\vx_n)) \\cdot ( 1 - \\sigma(\\vw^T \\phi(\\vx_n)) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "H_\\vw E(\\vw) \n",
    "&= \\sum \\nolimits_{n=1}^N \\phi(\\vx_n) r_n(\\vw) \\phi(\\vx_n)^T \\\\\n",
    "&= \\begin{bmatrix}\n",
    "| &  & | \\\\ \n",
    "\\phi(\\vx_1) & \\cdots & \\phi(\\vx_N)\\\\ \n",
    "| &  & |\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_1(\\vw) &  & \\\\ \n",
    " & \\ddots & \\\\ \n",
    " &  & r_N(\\vw)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "- & \\phi(\\vx_1)^T & -\\\\ \n",
    " & \\vdots & \\\\ \n",
    "- & \\phi(\\vx_N)^T & -\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\boxed{\\Phi^T R(\\vw) \\Phi}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- of which\n",
    "    $$\n",
    "    R(\\vw) = \\begin{bmatrix}\n",
    "    r_1(\\vw) &  &  & \\\\ \n",
    "     & r_2(\\vw) &  & \\\\ \n",
    "     &  & \\ddots & \\\\ \n",
    "     &  &  & r_N(\\vw)\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Applying Newton's Method\n",
    "\n",
    "- We already have\n",
    "    $$\n",
    "    \\begin{gather}\n",
    "    \\nabla_\\vw E(\\vw) = \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right) \\\\\n",
    "    H_\\vw E(\\vw) = \\Phi^T R(\\vw) \\Phi\n",
    "    \\end{gather}\n",
    "    $$\n",
    "    \n",
    "- So the iteration step is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\vw_{n+1}\n",
    "    &= \\vw_n - \\left(H_\\vw E(\\vw_n)\\right)^{-1} \\nabla_\\vw f(\\vw_n) \\\\\n",
    "    &= \\boxed{\\vw_n - \\left(\\Phi^T R(\\vw_n) \\Phi \\right)^{-1} \\Phi^T \\left( \\sigma(\\Phi \\vw_n) - \\vt \\right)}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- Repeat until convergence and we could get maximum likelihood estimate $\\vw_{ML}$ which minimizes the loss function $E(\\vw)$ and maximizes likelihood function $ P(\\vy = \\vt| \\mathcal{X}, \\vw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Do we have closed-form solution?\n",
    "\n",
    "- Recall for **ordinary least squares** and **regularized least squares**, we have closed-form solution:\n",
    "\n",
    "|               | Ordinary Least Squares  |  Regularized Least Squares |\n",
    "| ------------- | :-------------: | :-------------: |\n",
    "| **Derivate of Loss Function** | $\\Phi^T\\Phi \\vec{w} - \\Phi^T \\vec{t}$ | $(\\Phi^T \\Phi + \\lambda I)\\vec{w} - \\Phi^T \\vec{t}$ |\n",
    "| **Closed-form Solution**      | $(\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{t}$   | $(\\Phi^T \\Phi + \\lambda I)^{-1} \\Phi^T \\vec{t}$     |\n",
    "\n",
    "- They are obtained by finding the closed-form root of derivative of loss functionã€‚\n",
    "\n",
    "- For logistic regression, we have\n",
    "    $$\n",
    "    \\begin{gather}\n",
    "    \\nabla_\\vw E(\\vw) = 0 \\\\\n",
    "    \\Downarrow \\\\\n",
    "    \\Phi^T \\left( \\sigma(\\Phi \\vw) - \\vt \\right) = 0\n",
    "    \\end{gather}\n",
    "    $$\n",
    "    \n",
    "- Existence of sigmoid function makes $\\nabla_\\vw E(\\vw)$ **nonlinear** and no closed-form solution exists.\n",
    "\n",
    "- So we must **iterate**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Appendix: Multi-class Classification using Logistic Regression\n",
    "\n",
    "- We have seen sigmoid function enables us to do binary classification with logistic regression.\n",
    "\n",
    "- What if we want have multiple classes?\n",
    "\n",
    "- We will resort to **softmax** aka **normalized exponential** function\n",
    "\n",
    "- **Softmax Function**\n",
    "    $$\n",
    "    p_k = \\frac{\\exp(q_k)}{\\sum_j \\exp(q_j)}\n",
    "    $$\n",
    "    Given any real numbers $q_1, \\ldots, q_n$, we can generate a distribution on them using softmax function.\n",
    "    \n",
    "- Recall in binary case, we have\n",
    "    $$\n",
    "    P(y = 1 | \\vx,\\vw) = \\sigma(\\vw^T \\phi(\\vx))\n",
    "    $$\n",
    "    \n",
    "- For K-class classification, we define $\\mathcal{W} = {\\vw}_{k=1}^K$. \n",
    "\n",
    "- The probablity data $\\vx$ belongs to class $j$ is\n",
    "    $$\n",
    "    P(y = j | \\vx,\\mathcal{W}) = \\frac{\\exp(\\vw_j^T \\phi(\\vw))}{\\sum_{k=1}^{K} \\exp(\\vw_k^T \\phi(\\vw))}\n",
    "    $$\n",
    "    \n",
    "- We classify using\n",
    "    $$\n",
    "    y = \\underset{j \\in \\{1,\\dots, K\\}}{\\arg \\max} P(y = j | \\vx,\\mathcal{W})\n",
    "    $$\n",
    "    \n",
    "- Similarly, $\\mathcal{W} = {\\vw}_{k=1}^K$ is learned by maximizing likelihood function.\n",
    "\n",
    "- For details, please refer to [this](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
