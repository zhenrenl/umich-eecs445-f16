{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# INSTRUCTIONS:\n",
    "* Take a tag, look at seating chart\n",
    "* Go to the *seat you were assigned*\n",
    "* Place the tag on your desk - **we are checking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 445:  Introduction to Machine Learning\n",
    "\n",
    "## Hands-On Lecture 3:  Convex Optimization and Probability\n",
    "\n",
    "*Monday, September 19, 2016*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "> This hands-on lecture corresponds to material from **Lecture 03: Convex Optimization & Probability**.\n",
    "\n",
    "**Hands-on Exercises**\n",
    "* **Problem 1:**  Convex Functions\n",
    "* **Problem 2:**  Lagrange Duality\n",
    "* **Problem 3:**  Conditional Probability\n",
    "* **Problem 4:**  Expectations\n",
    "* **Problem 5:**  Variances\n",
    "* **Problem 6:**  Maximize Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 1:  Convex Functions\n",
    "\n",
    "(a) Prove $\\forall a, b \\in \\mathbb{R}$, $f(x) = ax + b$ is convex.\n",
    "\n",
    "(b) Prove $\\forall a \\geq 0, \\forall b, c \\in \\mathbb{R}$, $f(x) = ax^2 + bx + c$ is convex.\n",
    "\n",
    "*Hint:*  Use convex definition:\n",
    "* We say that a function $f$ is *convex* if, for any distinct pair of points $x_1,x_2$ we have\n",
    "$$f(tx_1+(1-t)x_2) \\leq tf(x_1)+(1-t)f(x_2) \\quad \\forall t \\in[0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 1:  Convex Functions\n",
    "\n",
    "(c) Prove the following function is convex\n",
    "$$f(x_1, x_2) = x_1^2 + x_2^2 + x_1x_2 = \\mathbf{x}^T\n",
    "\\begin{bmatrix}\n",
    "   1 & \\frac{1}{2} \\\\\n",
    "   \\frac{1}{2} & 1\n",
    "\\end{bmatrix}\\mathbf{x}, \\forall x_1, x_2 \\in \\mathbb{R}\n",
    "$$\n",
    "    \n",
    "*Hint:*  Use convex definition:\n",
    "* If $f$ is differentiable, then $f$ is convex iff $f(x + y) \\geq f(x) + \\nabla_x f(x) \\cdot y \\quad \\forall x,y$\n",
    "\n",
    "* If $f$ is twice-differentiable, then $f$ is convex iff its hessian is always positive semi-definite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 1: Convex Functions\n",
    "\n",
    "(c)\n",
    "$$ \n",
    "\\begin{array}{ll}\n",
    "& f(\\mathbf{x} + \\mathbf{y}) - f(\\mathbf{x}) - \\nabla_x f(\\mathbf{x})^T\\cdot \\mathbf{y}\\\\\n",
    "= & (x_1 + y_1)^2 + (x_2 + y_2) ^2 + (x_1 + y_1)(x_2 + y_2) - \\\\\n",
    "&(x_1^2 + x_2^2 + x_1x_2) - ((2x_1 + x_2)y_1 + (x_1 + 2x_2)y_2) \\\\\n",
    "= & y_1^2 + y_2^2 \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The Hessian is\n",
    "$\\textbf{H} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $\\forall \\mathbf{x} \\in \\mathbb{R}^2$, $\\mathbf{x}^TH\\mathbf{x} = 2x_1^2 + 2x_2^2 + 2x_1x_2 = x_1^2 + x_2^2 + (x_1 + x_2)^2 \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 2: Convex Optimization\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{maximize} & f(x) = x_1 + x_2 \\\\\n",
    "\\mbox{subject to} & 4x_1 - x_2 \\leq 8 \\\\\n",
    " & 2x_1 + x_2 \\leq 10 \\\\\n",
    " & 5x_1 - x_2 \\geq -2\n",
    "\\end{array}\n",
    "$$\n",
    "(a) Convert the above linear programming problem into the standard form.\n",
    "\n",
    "*Hint:* Standard form of primal problem: objective function should be **minimize**, constraints only contain \"$\\leq$\" and \"$=$\", **i.e.**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& {\\text{minimize}} & & f(\\mathbf{x})\\\\\n",
    "& \\text{subject to} & & g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, ..., m\\\\\n",
    "& & & h_j(x) = 0, \\quad j = 1, ..., n\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 2: Convex Optimization\n",
    "(a) $$\n",
    "\\begin{array}{lll}\n",
    "\\mbox{minimize} & f(x) = -x_1 - x_2& \\\\\n",
    "\\mbox{subject to} & 4x_1 - x_2 - 8 &\\leq 0 \\\\ & 2x_1 + x_2 - 10 &\\leq 0 \\\\ & -5x_1 + x_2 - 2 &\\leq 0 \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 2: Convex Optimization\n",
    "(b) Derive the dual problem.\n",
    "\n",
    "*Hint:* \n",
    "* Its Lagrangian is \n",
    "$L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)$\n",
    "\n",
    "* The **Langragian dual function** is $L_D(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\triangleq \\underset{x}{\\min}L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\\\ = \\underset{x}{\\min} \\ \\left[ f(x) + \\sum_{i=1}^m \\lambda_ig_i(x) + \\sum_{j=1}^n \\nu_j h_j(x) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 2: Convex Optimization\n",
    "(b) $$\n",
    "\\begin{array}{lll}\n",
    "L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) &=& f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x) \\\\\n",
    "&=& -x_1 - x_2 + \\lambda_1(4x_1 - x_2 - 8) + \\\\\n",
    "&&\\lambda_2(2x_1 + x_2 - 10) + \\lambda_3(-5x_1 + x_2 - 2) \\\\\n",
    "&=& x_1(4\\lambda_1 + 2\\lambda_2 - 5\\lambda_3 - 1) + x_2(-2\\lambda_1 + \\lambda_2 + \\\\\n",
    "&& \\lambda_3 - 1) + (-8\\lambda_1 - 10\\lambda_2 - 2\\lambda_3) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "Dual problem is\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mbox{maximize} & L_D(\\boldsymbol{\\lambda}) &= \\underset{x}{\\min}L(x,\\boldsymbol{\\lambda}) \\\\\n",
    "\\mbox{subject to} & \\lambda_i &\\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To get feasible solution, we must have the coefficients of $x_1$ and $x_2$ to be zero, otherwise $\\underset{x}{\\min}L(x,\\boldsymbol{\\lambda})$ would get negetive infinity:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "4\\lambda_1 + 2\\lambda_2 - 5\\lambda_3 - 1 &= 0 \\\\\n",
    "-2\\lambda_1 + \\lambda_2 + \\lambda_3 - 1 &= 0\n",
    "\\end{array}\n",
    "$$\n",
    "then $L_D(\\boldsymbol{\\lambda})$ becomes $-8\\lambda_1 - 10\\lambda_2 - 2\\lambda_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Axioms of Probability\n",
    "* $0 \\leq P(A) \\leq 1$\n",
    "* $P(\\Omega) = 1$\n",
    "* If $A_1, A_2, \\cdots, A_k$ are disjoint events, then$P(\\bigcup_{i=1}^{k}{A_i}) = \\sum_{i=1}^{k}{P(A_i)}$.\n",
    "\n",
    "### Conditional Probability\n",
    "For events $A, B \\in \\mathcal{F}$ with $P(B) > 0$, we may write the **conditional probability of A given B**:\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 3\n",
    "Suppose $x, y$ are discrete variables and the probability distribution is as follows:\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    " & x = 0 & x = 1 & x = 2 \\\\\n",
    " y = 0 & 0.1 & 0.2 & 0.3 \\\\\n",
    " y = 1 & 0.2 & 0.1 & 0.1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(a) Compute $p(x = 0)$, $p(y = 1)$.\n",
    "\n",
    "(b) Compute $p(x | y = 1)$.\n",
    "\n",
    "(c) Prove the Bayes' Rule\n",
    "$$P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j}{P(A|B_j)P(B_j)}} $$\n",
    "Where $B_i$ is a partition of $\\Omega$(note the bottom is just the law of probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 4\n",
    "Given a 2D Gaussian distribution\n",
    "$$p(x, y) = \\frac{1}{\n",
    "    \\sqrt{2}\\pi\n",
    "}\\exp\\{-\\frac{1}{2}(x^2 + xy + 2y^2)\\}$$\n",
    "(a) Compute $p(x = 4, y)$.\n",
    "\n",
    "(b) Compute $p(x = 4)$.\n",
    "\n",
    "(c) Using Bayse' Rule, compute $p(y \\mid x = 4)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 4\n",
    "\n",
    "(a) $$p(x = 4, y) = \\frac{1}{\n",
    "    \\sqrt{2}\\pi\n",
    "}\\exp\\{-\\frac{1}{2}(16 + 4y + 2y^2)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(b) $$\n",
    "\\begin{array}{lll}\n",
    "p(x = 4) &=& \\int_{y}{p(x = 4, y)} \\\\\n",
    "&=& \\int_{y}{\\frac{1}{\\sqrt{2}\\pi}\\exp\\{-\\frac{1}{2}(16 + 4y + 2y^2)\\}} \\\\\n",
    "&=& \\int_{y}{\\frac{1}{\\sqrt{2}\\pi}\\exp\\{-\\frac{1}{2}(14 + 2(y + 1)^2)\\}} \\\\\n",
    "&=& \\frac{1}{\\sqrt{2\\pi}}\\exp{(-7)}\\int_{y}{\\frac{1}{\\sqrt{\\pi}}\\exp\\{-\\frac{1}{2}(2(y + 1)^2)\\}} \\\\\n",
    "&=& \\frac{1}{\\sqrt{2\\pi}}\\exp{(-7)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Notice that $\\frac{1}{\\sqrt{\\pi}}\\exp\\{-\\frac{1}{2}(2(y + 1)^2)\\} = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\}$ is Normal distribution with $\\mu = -1$ and $\\sigma^2 = \\frac{1}{2}$, so the integration is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(c) $$p(y|x = 4) = \\frac{p(x = 4, y)}{p(x = 4)} = \\frac{1}{\\sqrt{\\pi}}\\exp\\{-\\frac{1}{2}(2(y + 1)^2)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Property of Expectation\n",
    "* $E[a] = a$ for any constant $a \\in \\mathbb{R}$.\n",
    "* $E[af(X)] = aE[f(X)]$ for any constant $a \\in \\mathbb{R}$.\n",
    "* $E[f(X) + g(X)] = E[f(X)] + E[g(X)]$.\n",
    "* For a discrete variable $X$, $E[1{X = k}] = P(X = k)$.\n",
    "\n",
    "### Problem 5\n",
    "(a) We have variance of a distribution: $Var[X] = E[X - E[X]]^2$, prove $Var[X] = E[X^2] - E[X]^2$.\n",
    "\n",
    "Then prove $Var[af(X)] = a^2Var[f(X)]$ for any constant $a \\in \\mathbb{R}$.\n",
    "\n",
    "(b) We also have multiple random variables $X$ and $Y$, then the covariance is defined as $Cov[X, Y] = E[(X - EX)(Y - EY)]$, prove $Cov[X, Y] = E[XY] - E[X]E[Y]$.\n",
    "\n",
    "(c) Prove $Var[X + Y] = Var[X] + Var[Y] + Cov[X, Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Maximum Likelihood\n",
    "Suppose we have a set of observed data $D$ and we want to evaluate a parameter setting $w$:\n",
    "$$p(w|D) = \\frac{p(D|w)p(w)}{p(D)}$$\n",
    "$$p(D) = \\sum_{w}{p(D|w)p(w)}$$\n",
    "\n",
    "We call $p(D|w)$ as the likelihood function. Then we have $p(w|D) \\propto p(D|w)p(w)$. Suppose $p(w)$ is the same for all $w$, we can only choose $w$ to maximize likelihood $p(D|w)$, which is to maximize the log-likelihood $\\log{p(D|w)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem 6: Maximum Likelihood Estimation\n",
    "We have observed data $x_1, \\cdots, x_n$ drawn from Bernoulli distribution:\n",
    "$$p(x) = \\begin{cases}\n",
    "    \\theta     & \\quad \\text{if } x = 1\\\\\n",
    "    1 - \\theta  & \\quad \\text{if } x = 0\\\\\n",
    "  \\end{cases}$$\n",
    "\n",
    "(a) What is the likelihood function based on $\\theta$?\n",
    "\n",
    "(b) What is the log-likelihood function?\n",
    "\n",
    "(c) Compute estimated $\\theta$ to maximize the log-likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 6: Maximum Likelihood Estimation\n",
    "(a) $$\n",
    "\\begin{array}{ll}\n",
    "L(\\theta) &= p(D|\\theta) = p(x_1, \\dots, x_n|\\theta) \\\\\n",
    "&= \\prod_{i}{p(x_i)} = \\theta^{\\sum{\\mathbb{1}(x_i = 1)}}(1 - \\theta)^{\\sum{\\mathbb{1}(x_i = 0)}} \\\\\n",
    "&= \\theta^{k}(1 - \\theta)^{n - k}\n",
    "\\end{array}\n",
    "$$\n",
    "where $k$ is the number of $1$s from the observed data.\n",
    "\n",
    "(b) $$\\log{L(\\theta)} = k\\log(\\theta) + (n - k)\\log(1 - \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution 6: Maximum Likelihood Estimation\n",
    "(c) Set the derivative of $log(L(\\theta))$ to zero we have\n",
    "$$\n",
    "\\frac{\\mathrm{d}log(L(\\theta))}{\\mathrm{d}\\theta} = \\frac{k}{\\theta} - \\frac{n - k}{1 - \\theta} = 0 \\\\\n",
    "\\frac{k}{\\theta} = \\frac{n - k}{1 - \\theta} \\\\\n",
    "\\theta = \\frac{k}{n}\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extra Problem: Maximum Likelihood Estimation for Gaussian Distribution\n",
    "We have observed data $x_1, \\cdots, x_n$ drawn from Normal distribution:\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^\\frac{1}{2}} \\exp{(-\\frac{1}{2\\sigma^2}(x - \\mu)^2)}$$\n",
    "\n",
    "(a) What is the likelihood function based on $\\mu$ and $\\sigma^2$?\n",
    "\n",
    "(b) What is the log-likelihood function?\n",
    "\n",
    "(c) Compute estimated parameters $\\mu$ and $\\sigma^2$ to maximize the log-likelihood function."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
