{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Thanks to [these notes](http://cs231n.github.io/convolutional-networks) for much of the material for this hands on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"700\"\n",
       "            src=\"http://cs231n.github.io//assets/conv-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb9343660b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('http://cs231n.github.io//assets/conv-demo/index.html', 850, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Conv Layer\n",
    "\n",
    "The example above serves as a visual guide to the parameters we are working with in a convolutional NN which:\n",
    "\n",
    "- Accepts a volume of size \\\\(W_1 \\times H_1 \\times D_1\\\\)\n",
    "- Requires four hyperparameters: \n",
    "  - Number of filters \\\\(K\\\\), \n",
    "  - their spatial extent \\\\(F\\\\), \n",
    "  - the stride \\\\(S\\\\), \n",
    "  - the amount of zero padding \\\\(P\\\\).\n",
    "- Produces a volume of size \\\\(W_2 \\times H_2 \\times D_2\\\\) where:\n",
    "  - \\\\(W_2 = (W_1 - F + 2P)/S + 1\\\\)\n",
    "  - \\\\(H_2 = (H_1 - F + 2P)/S + 1\\\\) (i.e. width and height are computed equally by symmetry)\n",
    "  - \\\\(D_2 = K\\\\)\n",
    "- With parameter sharing, it introduces \\\\(F \\cdot F \\cdot D_1\\\\) weights per filter, for a total of \\\\((F \\cdot F \\cdot D_1) \\cdot K\\\\) weights and \\\\(K\\\\) biases.\n",
    "- In the output volume, the \\\\(d\\\\)-th depth slice (of size \\\\(W_2 \\times H_2\\\\)) is the result of performing a valid convolution of the \\\\(d\\\\)-th filter over the input volume with a stride of \\\\(S\\\\), and then offset by \\\\(d\\\\)-th bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Local connectivity basics\n",
    "\n",
    "Lets' get familiar with the how a convolutional layer works, and reduces the number of required weights compared with a fully connected layer, by working through an example.\n",
    "\n",
    "Suppose we have a 32x32x3 input image with 10 3x3 filters of stride 1, pad 1. \n",
    "\n",
    "\n",
    "**Q1** How many neurons are in this layer?\n",
    "\n",
    "**Q2** How many weights would be required for a fully connected layer with this many neurons?\n",
    "\n",
    "**Q3** How many weights are required for each neuron in our convolutional layer? How many weights total for the layer, considering weight sharing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: implement forward propogation for a convolutional layer\n",
    "\n",
    "Let's implement forward propogation for a conv layer. Fill in the code below. There is an example with a solution output in place to help you see how close your solution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "    The input consists of N data points, each with C channels, height H and width\n",
    "    W. We convolve each input with F different filters, where each filter spans\n",
    "    all C channels and has height HH and width HH.\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    (N, C, H, W) = x.shape\n",
    "    (F, _, HH, WW) = w.shape\n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    H_prime = 1 + int((H + 2 * pad - HH) / stride)\n",
    "    W_prime = 1 + int((W + 2 * pad - WW) / stride)\n",
    "    out = np.zeros((N, F, H_prime, W_prime))\n",
    "\n",
    "    # your code goes here!\n",
    "    # hint: you can use the function np.pad for padding    \n",
    "\n",
    "\n",
    "    # end of your code\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "#\n",
    "# Test bed\n",
    "#\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216]],\n",
    "                          [[0.21027089, 0.21661097],\n",
    "                           [0.22847626, 0.23004637]],\n",
    "                          [[0.50813986, 0.54309974],\n",
    "                           [0.64082444, 0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[0.69108355, 0.66880383],\n",
    "                           [0.59480972, 0.56776003]],\n",
    "                          [[2.36270298, 2.36904306],\n",
    "                           [2.38090835, 2.38247847]]]]])\n",
    "\n",
    "# Compare your output to solution. Should be within 1e-8 (and print out 0.0000)\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: {:.4f}'.format(np.asscalar(rel_error(out, correct_out))))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
