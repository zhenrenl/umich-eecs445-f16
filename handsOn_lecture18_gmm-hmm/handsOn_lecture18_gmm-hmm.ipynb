{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\X}{\\mathcal{X}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\X}{\\mathcal{X}}\n",
    "\\newcommand{\\Parents}{\\mathrm{Parents}}\n",
    "\\newcommand{\\NonDesc}{\\mathrm{NonDesc}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\dsep}{\\text{d-sep}}\n",
    "\\newcommand{\\Cat}{\\mathrm{Categorical}}\n",
    "\\newcommand{\\Bin}{\\mathrm{Binomial}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HMMs and the Forward Backward Algorithm\n",
    "\n",
    "## Derivations\n",
    "\n",
    "Let's start by deriving the forward and backward algorithms from lecture. It's really important you understand how the recursion comes into play. We covered it in lecture, please try not to cheat during this exercise, you won't get anything out of it if you don't try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "Recall that a Hidden Markov Model (HMM) is a particular factorization of a joint distribution representing noisy observations $X_k$ generated from *discrete* hidden Markov chain $Z_k$.\n",
    "$$\n",
    "P(\\vec{X}, \\vec{Z}) = P(Z_1) P(X_1 \\mid Z_1) \\prod_{k=2}^T P(Z_k \\mid Z_{k-1}) P(X_k \\mid Z_k)\n",
    "$$\n",
    "\n",
    "and as a bayesian network:\n",
    "\n",
    "<img src=\"hmm.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM:  Parameters\n",
    "\n",
    "For a Hidden Markov Model with $N$ hidden states and $M$ observed states, there are three *row-stochastic* parameters $\\theta=(A,B,\\pi)$,\n",
    "- Transition matrix  $A \\in \\R^{N \\times N}$\n",
    "    $$\n",
    "    A_{ij} = p(Z_t = j | Z_{t-1} = i)\n",
    "    $$\n",
    "- Emission matrix $B \\in \\R^{N \\times M}$\n",
    "    $$\n",
    "    B_{jk} = p(X_t = k | Z_t = j)\n",
    "    $$\n",
    "- Initial distribution $\\pi \\in \\R^N$,\n",
    "    $$\n",
    "    \\pi_j = p(Z_1 = j)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM:  Filtering Problem\n",
    "\n",
    "**Filtering** means to compute the current *belief state* $p(z_t | x_1, \\dots, x_t,\\theta)$.\n",
    "    $$\n",
    "    p(z_t | x_1,\\dots,x_t) = \\frac{p(x_1,\\dots,x_t,z_t)}{p(x_1,\\dots,x_t)}\n",
    "    $$\n",
    "- Given observations $x_{1:t}$ so far, infer $z_t$.\n",
    "\n",
    "> Solved by the **forward algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we infer values of hidden variables?\n",
    "\n",
    "- One of the most challenging part of HMMs is to try to \"predict\" what are the values of the hidden variables $z_t$, having observed all the $x_1, \\ldots, x_T$.\n",
    "- Computing $p(z_t \\mid \\X)$ is known on *smoothing*. More on this soon.\n",
    "- But it turns out that this probability can be computed from two other quantities:\n",
    "    - $p(x_1,\\dots,x_t,z_t)$, which we are going to label $\\alpha_t(z_t)$\n",
    "    - $p(x_{t+1},\\dots,x_{T} | z_t)$, which we are going to label $\\beta_t(z_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem:  Derive the Forward Algorithm\n",
    "\n",
    "The **forward algorithm** computes $\\alpha_t(z_t) \\equiv p(x_1,\\dots,x_t,z_t)$.\n",
    "\n",
    "The challenge is to frame this in terms of $\\alpha_{t-1}(z_{t-1})$. We'll get you started by marginalizing over \"one step back\". You need to fill in the rest!\n",
    "\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\alpha_t(z_t)\n",
    "    &= \\sum_{z_{t-1}} p(x_1, \\dots, x_t, z_{t-1}, z_t) \\\\\n",
    "    \\dots \\\\\n",
    "    &= B_{z_t,x_t} \\sum_{z_{t-1}} \\alpha_{t-1}(z_{t-1}) A_{z_{t-1}, z_t}\n",
    "    \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hints:\n",
    "\n",
    "1. You are trying to pull out $\\alpha_{t-1}(z_{t-1}) = p(x_1, \\dots, x_{k-1}, z_{t-1})$ Can you factor out $p(x_k)$ and $p(z_k)$ using bayes theorem ($P(A,B) = P(A|B)P(B)$)?\n",
    "2. Once you do, conditional independence (look for d-separation!) should help simplify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Derive the Backward Algorithm\n",
    "\n",
    "The **backward algorithm** computes $\\beta_t(z_t) \\equiv p(x_{t+1},\\dots,x_{T} | z_t)$\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\beta(z_t)\n",
    "    &= \\sum_{z_{t+1}} p(x_{t+1},\\dots,x_{T},z_{t+1} | z_t) \\\\\n",
    "    \\dots \\\\\n",
    "    &= \\sum_{z_{t+1}} A_{z_t, z_{t+1}} B_{z_{t+1}, x_{t+1}} \\beta_{t+1}(z_{t+1})\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Similar to deriving the forward algorithm, we've gotten you started by marginalizing over \"one step forward\". Use applications of bayes rule $P(A,B) = P(A|B)P(B)$ and simplifications from conditional independence to get the rest of the way there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code example: part of speech tagger\n",
    "\n",
    "Now that we're comfortable with the theory behind the forward and backward algorithm, let's set up a real example and implement both procedures.\n",
    "\n",
    "In this example, we observe a sequence of words backed by a latent part of speech variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$X$: discrete distribution over bag of words\n",
    "\n",
    "$Z$: discrete distribution over parts of speech\n",
    "\n",
    "$A$: the probability of a part of speech given a previous part of speech, e.g, what do we expect to see after a noun? \n",
    "\n",
    "$B$: the distribution of words given a particular part of speech, e.g, what words are we likely to see if we know it is a verb?\n",
    "\n",
    "$x_{i}s$ a sequence of observed words (a sentence). Note: in for both variables we have a special \"end\" outcome that signals the end of a sentence. This makes sense as a part of speech tagger would like to have a sense of sentence boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "parts_of_speech = DETERMINER, NOUN, VERB, END = 0, 1, 2, 3\n",
    "words = THE, DOG, WALKED, IN, PARK, END = 0, 1, 2, 3, 4, 5\n",
    "\n",
    "# transition probabilities\n",
    "A = np.array([\n",
    "        # D     N   V   E\n",
    "        [0.1, 0.8, 0.1, 0.0],  # D: determiner most likely to go to noun\n",
    "        [0.1, 0.1, 0.6, 0.2],  # N: noun most likely to go to verb\n",
    "        [0.4, 0.3, 0.2, 0.1],  # V \n",
    "        [0.0, 0.0, 0.0, 1.0]]) # E: end always goes to end\n",
    "\n",
    "# distribution of parts of speech for the first word of a sentence\n",
    "pi = np.array([0.4, 0.3, 0.3, 0.0])\n",
    "\n",
    "# emission probabilities\n",
    "B = np.array([\n",
    "        # D     N     V     E\n",
    "        [ 0.8,  0.1,  0.1,  0. ],  # the\n",
    "        [ 0.1,  0.8,  0.1,  0. ],  # dog\n",
    "        [ 0. ,  0. ,  1. ,  0. ],  # walked\n",
    "        [ 1. ,  0. ,  0. ,  0. ],  # in\n",
    "        [ 0. ,  0.1,  0.9,  0. ],  # park\n",
    "        [ 0. ,  0. ,  0. ,  1. ]]) # end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Implement the Forward Algorithm\n",
    "\n",
    "Now it's time to put it all together. We create a table to hold the results and build them up from the front to back.\n",
    "\n",
    "Along with the results, we return the marginal probability that can be compared with the backward algorithm's below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.32,  0.03,  0.03,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.  ,  0.  ,  0.  ,  0.  ]]), 0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def forward(params, observations):\n",
    "    pi, A, B = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    # p(z1) * p(x1|z1)\n",
    "    alpha[0, :] = pi * B[observations[0], :]\n",
    "    \n",
    "    # recursive case - YOUR CODE GOES HERE\n",
    "    \n",
    "    return (alpha, np.sum(alpha[N-1,:]))\n",
    "\n",
    "forward((pi, A, B), [THE, DOG, WALKED, IN, THE, PARK, END])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Implement the Backward Algorithm\n",
    "\n",
    "If you implemented both correctly, the second return value (the marginals) from each method should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.]]), 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backward(params, observations):\n",
    "    pi, A, B = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    beta = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    beta[N-1, :] = 1\n",
    "    \n",
    "    # recursive case -- YOUR CODE GOES HERE!    \n",
    "    \n",
    "    return (beta, np.sum(pi * B[observations[0], :] * beta[0,:]))\n",
    "\n",
    "backward((pi, A, B), [THE, DOG, WALKED, IN, THE, PARK, END])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
